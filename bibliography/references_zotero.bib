
@misc{3DMatchLearningLocal,
  title = {{{3DMatch}}: {{Learning Local Geometric Descriptors}} from {{RGB}}-{{D Reconstructions}}},
  file = {/home/akashsharma/Zotero/storage/J2SWAJV9/3dmatch.cs.princeton.edu.html},
  howpublished = {http://3dmatch.cs.princeton.edu/}
}

@article{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  month = dec,
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  archiveprefix = {arXiv},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/AP34Q3DZ/Arjovsky et al. - 2017 - Wasserstein GAN.pdf;/home/akashsharma/Zotero/storage/F6NXHKC9/1701.html},
  journal = {arXiv:1701.07875 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/home/akashsharma/Zotero/storage/8XIY8F2N/Bishop - 2006 - Pattern recognition and machine learning.pdf},
  isbn = {978-0-387-31073-2},
  keywords = {Machine learning,Pattern perception},
  language = {en},
  lccn = {Q327 .B52 2006},
  series = {Information Science and Statistics}
}

@article{blackUnificationLineProcesses1996,
  title = {On the Unification of Line Processes, Outlier Rejection, and Robust Statistics with Applications in Early Vision},
  author = {Black, Michael J. and Rangarajan, Anand},
  year = {1996},
  month = jul,
  volume = {19},
  pages = {57--91},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/BF00131148},
  abstract = {The modeling of spatial discontinuities for problems such as surface recovery, segmentation, image reconstruction, and optical flow has been intensely studied in computer vision. While ``line-process'' models of discontinuities have received a great deal of attention, there has been recent interest in the use of robust statistical techniques to account for discontinuities. This paper unifies the two approaches. To achieve this we generalize the notion of a ``line process'' to that of an analog ``outlier process'' and show how a problem formulated in terms of outlier processes can be viewed in terms of robust statistics. We also characterize a class of robust statistical problems for which an equivalent outlier-process formulation exists and give a straightforward method for converting a robust estimation problem into an outlier-process formulation. We show how prior assumptions about the spatial structure of outliers can be expressed as constraints on the recovered analog outlier processes and how traditional continuation methods can be extended to the explicit outlier-process formulation. These results indicate that the outlierprocesses approach provides a general framework which subsumes the traditional line-process approaches as well as a wide class of robust estimation problems. Examples in surface reconstruction, image segmentation, and optical flow are presented to illustrate the use of outlier processes and to show how the relationship between outlier processes and robust statistics can be exploited. An appendix provides a catalog of common robust error norms and their equivalent outlier-process formulations.},
  file = {/home/akashsharma/Zotero/storage/YR39DPYK/Black and Rangarajan - 1996 - On the unification of line processes, outlier reje.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {1}
}

@inproceedings{bowmanProbabilisticDataAssociation2017,
  title = {Probabilistic Data Association for Semantic {{SLAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bowman, Sean L. and Atanasov, Nikolay and Daniilidis, Kostas and Pappas, George J.},
  year = {2017},
  month = may,
  pages = {1722--1729},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/ICRA.2017.7989203},
  abstract = {Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.},
  file = {/home/akashsharma/Zotero/storage/5BN5XH69/Bowman et al. - 2017 - Probabilistic data association for semantic SLAM.pdf},
  isbn = {978-1-5090-4633-1},
  language = {en}
}

@book{boydConvexOptimization2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  file = {/home/akashsharma/Zotero/storage/E237LMKE/Boyd and Vandenberghe - 2004 - Convex optimization.pdf},
  isbn = {978-0-521-83378-3},
  keywords = {Convex functions,Mathematical optimization},
  language = {en},
  lccn = {QA402.5 .B69 2004}
}

@article{brachmannDSACDifferentiableRANSAC2018,
  title = {{{DSAC}} - {{Differentiable RANSAC}} for {{Camera Localization}}},
  author = {Brachmann, Eric and Krull, Alexander and Nowozin, Sebastian and Shotton, Jamie and Michel, Frank and Gumhold, Stefan and Rother, Carsten},
  year = {2018},
  month = mar,
  abstract = {RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.},
  archiveprefix = {arXiv},
  eprint = {1611.05705},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/QCE5EDYB/Brachmann et al. - 2018 - DSAC - Differentiable RANSAC for Camera Localizati.pdf;/home/akashsharma/Zotero/storage/3RNZXZ4J/1611.html},
  journal = {arXiv:1611.05705 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{brachmannNeuralGuidedRANSACLearning2019,
  title = {Neural-{{Guided RANSAC}}: {{Learning Where}} to {{Sample Model Hypotheses}}},
  shorttitle = {Neural-{{Guided RANSAC}}},
  author = {Brachmann, Eric and Rother, Carsten},
  year = {2019},
  month = jul,
  abstract = {We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.},
  archiveprefix = {arXiv},
  eprint = {1905.04132},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/YYVFHRIG/Brachmann and Rother - 2019 - Neural-Guided RANSAC Learning Where to Sample Mod.pdf;/home/akashsharma/Zotero/storage/ZEMVBDNL/1905.html},
  journal = {arXiv:1905.04132 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{bylowRealTimeCameraTracking2013,
  title = {Real-{{Time Camera Tracking}} and {{3D Reconstruction Using Signed Distance Functions}}},
  booktitle = {Robotics: {{Science}} and {{Systems IX}}},
  author = {Bylow, Erik and Sturm, J{\"u}rgen and Kerl, Christian and Kahl, Fredrik and Cremers, Daniel},
  year = {2013},
  month = jun,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2013.IX.035},
  abstract = {The ability to quickly acquire 3D models is an essential capability needed in many disciplines including robotics, computer vision, geodesy, and architecture. In this paper we present a novel method for real-time camera tracking and 3D reconstruction of static indoor environments using an RGB-D sensor. We show that by representing the geometry with a signed distance function (SDF), the camera pose can be efficiently estimated by directly minimizing the error of the depth images on the SDF. As the SDF contains the distances to the surface for each voxel, the pose optimization can be carried out extremely fast. By iteratively estimating the camera poses and integrating the RGB-D data in the voxel grid, a detailed reconstruction of an indoor environment can be achieved. We present reconstructions of several rooms using a hand-held sensor and from onboard an autonomous quadrocopter. Our extensive evaluation on publicly available benchmark data shows that our approach is more accurate and robust than the iterated closest point algorithm (ICP) used by KinectFusion, and yields often a comparable accuracy at much higher speed to feature-based bundle adjustment methods such as RGB-D SLAM for up to medium-sized scenes.},
  file = {/home/akashsharma/Zotero/storage/6L4PN33X/Bylow et al. - 2013 - Real-Time Camera Tracking and 3D Reconstruction Us.pdf},
  isbn = {978-981-07-3937-9},
  language = {en}
}

@article{carloneInitializationTechniques3D,
  title = {Initialization {{Techniques}} for {{3D SLAM}}: A {{Survey}} on {{Rotation Estimation}} and Its {{Use}} in {{Pose Graph Optimization}}},
  author = {Carlone, Luca and Tron, Roberto and Daniilidis, Kostas and Dellaert, Frank},
  pages = {8},
  abstract = {Pose graph optimization is the non-convex optimization problem underlying pose-based Simultaneous Localization and Mapping (SLAM). If robot orientations were known, pose graph optimization would be a linear leastsquares problem, whose solution can be computed efficiently and reliably. Since rotations are the actual reason why SLAM is a difficult problem, in this work we survey techniques for 3D rotation estimation. Rotation estimation has a rich history in three scientific communities: robotics, computer vision, and control theory. We review relevant contributions across these communities, assess their practical use in the SLAM domain, and benchmark their performance on representative SLAM problems (Fig. 1). We show that the use of rotation estimation to bootstrap iterative pose graph solvers entails significant boost in convergence speed and robustness.},
  file = {/home/akashsharma/Zotero/storage/CJC5NSPY/Carlone et al. - Initialization Techniques for 3D SLAM a Survey on.pdf},
  language = {en}
}

@misc{CeresSolverLarge,
  title = {Ceres {{Solver}} \textemdash{} {{A Large Scale Non}}-Linear {{Optimization Library}}},
  file = {/home/akashsharma/Zotero/storage/ESGUZ2H3/ceres-solver.org.html},
  howpublished = {http://ceres-solver.org/}
}

@article{chabraDeepLocalShapes2020,
  title = {Deep {{Local Shapes}}: {{Learning Local SDF Priors}} for {{Detailed 3D Reconstruction}}},
  shorttitle = {Deep {{Local Shapes}}},
  author = {Chabra, Rohan and Lenssen, Jan Eric and Ilg, Eddy and Schmidt, Tanner and Straub, Julian and Lovegrove, Steven and Newcombe, Richard},
  year = {2020},
  month = apr,
  abstract = {Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.},
  archiveprefix = {arXiv},
  eprint = {2003.10983},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/HF94CTFE/Chabra et al. - 2020 - Deep Local Shapes Learning Local SDF Priors for D.pdf;/home/akashsharma/Zotero/storage/MYHUSJ7A/Chabra et al. - 2020 - Deep Local Shapes Learning Local SDF Priors for D.pdf;/home/akashsharma/Zotero/storage/H3283HCR/2003.html;/home/akashsharma/Zotero/storage/KDDLNXT7/2003.html},
  journal = {arXiv:2003.10983 [cs]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{chaplotLEARNINGEXPLOREUSING2020,
  title = {{{LEARNING TO EXPLORE USING ACTIVE NEURAL SLAM}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Saurabh and Gupta, Abhinav and Salakhutdinov, Ruslan},
  year = {2020},
  pages = {18},
  abstract = {This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.},
  file = {/home/akashsharma/Zotero/storage/36XUFW33/Chaplot et al. - 2020 - LEARNING TO EXPLORE USING ACTIVE NEURAL SLAM.pdf},
  language = {en}
}

@article{chenBSPNetGeneratingCompact2020,
  title = {{{BSP}}-{{Net}}: {{Generating Compact Meshes}} via {{Binary Space Partitioning}}},
  shorttitle = {{{BSP}}-{{Net}}},
  author = {Chen, Zhiqin and Tagliasacchi, Andrea and Zhang, Hao},
  year = {2020},
  month = jun,
  abstract = {Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-of-the-art methods while using much fewer primitives. Code is available at https://github.com/czq142857/BSP-NET-original.},
  archiveprefix = {arXiv},
  eprint = {1911.06971},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/7FCB9DX2/Chen et al. - 2020 - BSP-Net Generating Compact Meshes via Binary Spac.pdf;/home/akashsharma/Zotero/storage/I5HPNB3I/1911.html},
  journal = {arXiv:1911.06971 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/37INRH2P/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf;/home/akashsharma/Zotero/storage/NQZMMQ9B/1806.html},
  journal = {arXiv:1806.07366 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{choyDeepGlobalRegistration2020,
  title = {Deep {{Global Registration}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Choy, Christopher and Dong, Wei and Koltun, Vladlen},
  year = {2020},
  month = jun,
  pages = {2511--2520},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00259},
  abstract = {We present Deep Global Registration, a differentiable framework for pairwise registration of real-world 3D scans. Deep global registration is based on three modules: a 6-dimensional convolutional network for correspondence confidence prediction, a differentiable Weighted Procrustes algorithm for closed-form pose estimation, and a robust gradient-based SE(3) optimizer for pose refinement. Experiments demonstrate that our approach outperforms stateof-the-art methods, both learning-based and classical, on real-world data.},
  file = {/home/akashsharma/Zotero/storage/2WLP8VKY/Choy et al. - 2020 - Deep Global Registration.pdf},
  isbn = {978-1-72817-168-5},
  language = {en}
}

@article{claracoTutorialSETransformation,
  title = {A Tutorial on {{SE}}(3) Transformation Parameterizations and on-Manifold Optimization},
  author = {Claraco, Jose Luis Blanco},
  pages = {66},
  file = {/home/akashsharma/Zotero/storage/7JJZCCWB/Claraco - A tutorial on SE(3) transformation parameterizatio.pdf;/home/akashsharma/Zotero/storage/FRHB3KZG/Claraco - A tutorial on SE(3) transformation parameterizatio.pdf},
  journal = {Technical report},
  language = {en}
}

@article{cristofaloGeoDConsensusbasedGeodesic2020,
  title = {{{GeoD}}: {{Consensus}}-Based {{Geodesic Distributed Pose Graph Optimization}}},
  shorttitle = {{{GeoD}}},
  author = {Cristofalo, Eric and Montijano, Eduardo and Schwager, Mac},
  year = {2020},
  month = sep,
  abstract = {We present a consensus-based distributed pose graph optimization algorithm for obtaining an estimate of the 3D translation and rotation of each pose in a pose graph, given noisy relative measurements between poses. The algorithm, called GeoD, implements a continuous time distributed consensus protocol to minimize the geodesic pose graph error. GeoD is distributed over the pose graph itself, with a separate computation thread for each node in the graph, and messages are passed only between neighboring nodes in the graph. We leverage tools from Lyapunov theory and multi-agent consensus to prove the convergence of the algorithm. We identify two new consistency conditions sufficient for convergence: pairwise consistency of relative rotation measurements, and minimal consistency of relative translation measurements. GeoD incorporates a simple one step distributed initialization to satisfy both conditions. We demonstrate GeoD on simulated and real world SLAM datasets. We compare to a centralized pose graph optimizer with an optimality certificate (SE-Sync) and a Distributed Gauss-Seidel (DGS) method. On average, GeoD converges 20 times more quickly than DGS to a value with 3.4 times less error when compared to the global minimum provided by SE-Sync. GeoD scales more favorably with graph size than DGS, converging over 100 times faster on graphs larger than 1000 poses. Lastly, we test GeoD on a multi-UAV vision-based SLAM scenario, where the UAVs estimate their pose trajectories in a distributed manner using the relative poses extracted from their on board camera images. We show qualitative performance that is better than either the centralized SE-Sync or the distributed DGS methods.},
  archiveprefix = {arXiv},
  eprint = {2010.00156},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/RC7JRJ28/Cristofalo et al. - 2020 - GeoD Consensus-based Geodesic Distributed Pose Gr.pdf;/home/akashsharma/Zotero/storage/L9WNXQLQ/2010.html},
  journal = {arXiv:2010.00156 [cs, eess]},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  primaryclass = {cs, eess}
}

@inproceedings{cunninghamDDFSAMConsistentDistributed2013,
  title = {{{DDF}}-{{SAM}} 2.0: {{Consistent}} Distributed Smoothing and Mapping},
  shorttitle = {{{DDF}}-{{SAM}} 2.0},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Cunningham, Alexander and Indelman, Vadim and Dellaert, Frank},
  year = {2013},
  month = may,
  pages = {5220--5227},
  publisher = {{IEEE}},
  address = {{Karlsruhe, Germany}},
  doi = {10.1109/ICRA.2013.6631323},
  abstract = {This paper presents an consistent decentralized data fusion approach for robust multi-robot SLAM in dangerous, unknown environments. The DDF-SAM 2.0 approach extends our previous work by combining local and neighborhood information in a single, consistent augmented local map, without the overly conservative approach to avoiding information double-counting in the previous DDF-SAM algorithm. We introduce the anti-factor as a means to subtract information in graphical SLAM systems, and illustrate its use to both replace information in an incremental solver and to cancel out neighborhood information from shared summarized maps. This paper presents and compares three summarization techniques, with two exact approaches and an approximation. We evaluated the proposed system in a synthetic example and show the augmented local system and the associated summarization technique do not double-count information, while keeping performance tractable.},
  file = {/home/akashsharma/Zotero/storage/CH2453CW/Cunningham et al. - 2013 - DDF-SAM 2.0 Consistent distributed smoothing and .pdf},
  isbn = {978-1-4673-5643-5 978-1-4673-5641-1},
  language = {en}
}

@inproceedings{curlessVolumetricMethodBuilding1996,
  title = {A Volumetric Method for Building Complex Models from Range Images},
  booktitle = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '96},
  author = {Curless, Brian and Levoy, Marc},
  year = {1996},
  pages = {303--312},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/237170.237269},
  abstract = {A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties.},
  file = {/home/akashsharma/Zotero/storage/DKHQAZVH/Curless and Levoy - 1996 - A volumetric method for building complex models fr.pdf},
  isbn = {978-0-89791-746-9},
  language = {en}
}

@article{czarnowskiDeepFactorsRealTimeProbabilistic2020,
  title = {{{DeepFactors}}: {{Real}}-{{Time Probabilistic Dense Monocular SLAM}}},
  shorttitle = {{{DeepFactors}}},
  author = {Czarnowski, Jan and Laidlow, Tristan and Clark, Ronald and Davison, Andrew J.},
  year = {2020},
  month = apr,
  volume = {5},
  pages = {721--728},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2020.2965415},
  abstract = {The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry.},
  file = {/home/akashsharma/Zotero/storage/DSBXVDHR/Czarnowski et al. - 2020 - DeepFactors Real-Time Probabilistic Dense Monocul.pdf;/home/akashsharma/Zotero/storage/RAMNAPXP/Czarnowski et al. - 2020 - DeepFactors Real-Time Probabilistic Dense Monocul.pdf},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  number = {2}
}

@article{daiBundleFusionRealtimeGlobally2017,
  title = {{{BundleFusion}}: {{Real}}-Time Globally Consistent {{3D}} Reconstruction Using on-the-Fly Surface Re-Integration},
  author = {Dai, Angela and Nie{\ss}ner, Matthias and Zoll{\"o}fer, Michael and Izadi, Shahram and Theobalt, Christian},
  year = {2017},
  journal = {ACM Transactions on Graphics 2017 (TOG)}
}

@article{davisAlgorithm8xxCOLAMD,
  title = {Algorithm 8xx: {{COLAMD}}, a Column Approximate Minimum Degree Ordering Algorithm},
  author = {Davis, Timothy A and Larimore, Stefan I and Gilbert, John R and Ng, Esmond G},
  pages = {5},
  file = {/home/akashsharma/Zotero/storage/HTJM9IAE/Davis et al. - Algorithm 8xx COLAMD, a column approximate minimu.pdf},
  language = {en}
}

@misc{DeepEquilibriumModels,
  title = {Deep Equilibrium Models - {{Google Search}}},
  file = {/home/akashsharma/Zotero/storage/N43ULXPD/search.html},
  howpublished = {https://www.google.com/search?q=deep+equilibrium+models\&oq=deep+equilibrium+models\&aqs=chrome..69i57j0l3j0i22i30l2j69i61.5174j0j4\&sourceid=chrome\&ie=UTF-8}
}

@article{dellaertFactorGraphsRobot2017,
  title = {Factor {{Graphs}} for {{Robot Perception}}},
  author = {Dellaert, Frank and Kaess, Michael},
  year = {2017},
  volume = {6},
  pages = {1--139},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000043},
  file = {/home/akashsharma/Zotero/storage/29IEWZMI/Dellaert and Kaess - 2017 - Factor Graphs for Robot Perception.pdf},
  journal = {Foundations and Trends in Robotics},
  language = {en},
  number = {1-2}
}

@article{dellaertSquareRootSAM2006,
  title = {Square {{Root SAM}}: {{Simultaneous Localization}} and {{Mapping}} via {{Square Root Information Smoothing}}},
  shorttitle = {Square {{Root SAM}}},
  author = {Dellaert, Frank and Kaess, Michael},
  year = {2006},
  month = dec,
  volume = {25},
  pages = {1181--1203},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364906072768},
  abstract = {Solving the SLAM problem is one way to enable a robot to explore, map, and navigate in a previously unknown environment. We investigate smoothing approaches as a viable alternative to extended Kalman filter-based solutions to the problem. In particular, we look at approaches that factorize either the associated information matrix or the measurement Jacobian into square root form. Such techniques have several significant advantages over the EKF: they are faster yet exact, they can be used in either batch or incremental mode, are better equipped to deal with non-linear process and measurement models, and yield the entire robot trajectory, at lower cost for a large class of SLAM problems. In addition, in an indirect but dramatic way, column ordering heuristics automatically exploit the locality inherent in the geographic nature of the SLAM problem. In this paper we present the theory underlying these methods, along with an interpretation of factorization in terms of the graphical model associated with the SLAM problem. We present both simulation results and actual SLAM experiments in large-scale environments that underscore the potential of these methods as an alternative to EKF-based approaches.},
  file = {/home/akashsharma/Zotero/storage/P83GUFNF/Dellaert and Kaess - 2006 - Square Root SAM Simultaneous Localization and Map.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {12}
}

@article{diebelRepresentingAttitudeEuler,
  title = {Representing {{Attitude}}: {{Euler Angles}}, {{Unit Quaternions}}, and {{Rotation Vectors}}},
  author = {Diebel, James},
  pages = {35},
  abstract = {We present the three main mathematical constructs used to represent the attitude of a rigid body in threedimensional space. These are (1) the rotation matrix, (2) a triple of Euler angles, and (3) the unit quaternion. To these we add a fourth, the rotation vector, which has many of the benefits of both Euler angles and quaternions, but neither the singularities of the former, nor the quadratic constraint of the latter. There are several other subsidiary representations, such as Cayley-Klein parameters and the axis-angle representation, whose relations to the three main representations are also described. Our exposition is catered to those who seek a thorough and unified reference on the whole subject; detailed derivations of some results are not presented.},
  file = {/home/akashsharma/Zotero/storage/9I3YRC26/Diebel - Representing Attitude Euler Angles, Unit Quaterni.pdf},
  language = {en}
}

@inproceedings{dongGPUAcceleratedRobust2019,
  title = {{{GPU Accelerated Robust Scene Reconstruction}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Dong, Wei and Park, Jaesik and Yang, Yi and Kaess, Michael},
  year = {2019},
  month = nov,
  pages = {7863--7870},
  publisher = {{IEEE}},
  address = {{Macau, China}},
  doi = {10.1109/IROS40897.2019.8967693},
  abstract = {We propose a fast and accurate 3D reconstruction system that takes a sequence of RGB-D frames and produces a globally consistent camera trajectory and a dense 3D geometry. We redesign core modules of a state-of-the-art offline reconstruction pipeline to maximally exploit the power of GPU. We introduce GPU accelerated core modules that include RGBD odometry, geometric feature extraction and matching, point cloud registration, volumetric integration, and mesh extraction. Therefore, while being able to reproduce the results of the highfidelity offline reconstruction system, our system runs more than 10 times faster on average. Nearly 10Hz can be achieved in medium size indoor scenes, making our offline system even comparable to online Simultaneous Localization and Mapping (SLAM) systems in terms of the speed. Experimental results show that our system produces more accurate results than several state-of-the-art online systems. The system is open source at https://github.com/theNded/Open3D.},
  file = {/home/akashsharma/Zotero/storage/8SMJWWEN/Dong et al. - 2019 - GPU Accelerated Robust Scene Reconstruction.pdf},
  isbn = {978-1-72814-004-9},
  language = {en}
}

@article{engelDirectSparseOdometry2018,
  title = {Direct {{Sparse Odometry}}},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  year = {2018},
  month = mar,
  volume = {40},
  pages = {611--625},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2017.2658577},
  abstract = {We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry \textendash represented as inverse depth in a reference frame \textendash{} and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  file = {/home/akashsharma/Zotero/storage/2APP8R7D/Engel et al. - 2018 - Direct Sparse Odometry.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {3}
}

@article{engelLSDSLAMLargeScaleDirect,
  title = {{{LSD}}-{{SLAM}}: {{Large}}-{{Scale Direct Monocular SLAM}}},
  author = {Engel, Jakob and Schops, Thomas and Cremers, Daniel},
  pages = {16},
  abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
  file = {/home/akashsharma/Zotero/storage/A8LX436A/Engel et al. - LSD-SLAM Large-Scale Direct Monocular SLAM.pdf},
  language = {en}
}

@article{estevesLearningEquivariantRepresentations2020,
  title = {Learning {{SO}}(3) {{Equivariant Representations}} with {{Spherical CNNs}}},
  author = {Esteves, Carlos and {Allen-Blanchette}, Christine and Makadia, Ameesh and Daniilidis, Kostas},
  year = {2020},
  volume = {128},
  issn = {0920-5691},
  doi = {10.1007/s11263-019-01220-1},
  abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard 3D shape retrieval and classification benchmarks.},
  file = {/home/akashsharma/Zotero/storage/ZCGHR94N/Esteves et al. - 2020 - Learning SO(3) Equivariant Representations with Sp.pdf;/home/akashsharma/Zotero/storage/WXJFT8AT/10.html},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {3}
}

@article{fathianCLEARConsistentLifting2020,
  title = {{{CLEAR}}: {{A Consistent Lifting}}, {{Embedding}}, and {{Alignment Rectification Algorithm}} for {{Multi}}-{{View Data Association}}},
  shorttitle = {{{CLEAR}}},
  author = {Fathian, Kaveh and Khosoussi, Kasra and Tian, Yulun and Lusk, Parker and How, Jonathan P.},
  year = {2020},
  month = mar,
  abstract = {Many robotics applications require alignment and fusion of observations obtained at multiple views to form a global model of the environment. Multi-way data association methods provide a mechanism to improve alignment accuracy of pairwise associations and ensure their consistency. However, existing methods that solve this computationally challenging problem are often too slow for real-time applications. Furthermore, some of the existing techniques can violate the cycle consistency principle, thus drastically reducing the fusion accuracy. This work presents the CLEAR (Consistent Lifting, Embedding, and Alignment Rectification) algorithm to address these issues. By leveraging insights from the multi-way matching and spectral graph clustering literature, CLEAR provides cycle consistent and accurate solutions in a computationally efficient manner. Numerical experiments on both synthetic and real datasets are carried out to demonstrate the scalability and superior performance of our algorithm in real-world problems. This algorithmic framework can provide significant improvement in the accuracy and efficiency of existing discrete assignment problems, which traditionally use pairwise (but potentially inconsistent) correspondences. An implementation of CLEAR is made publicly available online.},
  archiveprefix = {arXiv},
  eprint = {1902.02256},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/9MILMZNU/Fathian et al. - 2020 - CLEAR A Consistent Lifting, Embedding, and Alignm.pdf;/home/akashsharma/Zotero/storage/6R4CYVUD/1902.html},
  journal = {arXiv:1902.02256 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multiagent Systems,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{feyDEEPGRAPHMATCHING2020,
  title = {{{DEEP GRAPH MATCHING CONSENSUS}}},
  author = {Fey, Matthias and Lenssen, Jan E and Morris, Christopher and Masci, Jonathan and Kriege, Nils M},
  year = {2020},
  pages = {23},
  abstract = {This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently. We demonstrate the practical effectiveness of our method on real-world tasks from the fields of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-theart. Our source code is available under https://github.com/rusty1s/ deep-graph-matching-consensus.},
  file = {/home/akashsharma/Zotero/storage/FZR6XHPS/Fey et al. - 2020 - DEEP GRAPH MATCHING CONSENSUS.pdf},
  language = {en}
}

@inproceedings{finmanEfficientIncrementalMap2014,
  title = {Efficient Incremental Map Segmentation in Dense {{RGB}}-{{D}} Maps},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Finman, Ross and Whelan, Thomas and Kaess, Michael and Leonard, John J.},
  year = {2014},
  month = may,
  pages = {5488--5494},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907666},
  abstract = {In this paper we present a method for incrementally segmenting large RGB-D maps as they are being created. Recent advances in dense RGB-D mapping have led to maps of increasing size and density. Segmentation of these raw maps is a first step for higher-level tasks such as object detection. Current popular methods of segmentation scale linearly with the size of the map and generally include all points. Our method takes a previously segmented map and segments new data added to that map incrementally online. Segments in the existing map are re-segmented with the new data based on an iterative voting method. Our segmentation method works in maps with loops to combine partial segmentations from each traversal into a complete segmentation model. We verify our algorithm on multiple real-world datasets spanning many meters and millions of points in real-time. We compare our method against a popular batch segmentation method for accuracy and timing complexity.},
  file = {/home/akashsharma/Zotero/storage/SDXGXTDV/Finman et al. - 2014 - Efficient incremental map segmentation in dense RG.pdf;/home/akashsharma/Zotero/storage/U2XM3EYQ/6907666.html},
  keywords = {autonomous systems,batch segmentation method,Complexity theory,dense RGB-D simultaneous localization-and-mapping,image colour analysis,image segmentation,Image segmentation,incremental map segmentation,iterative methods,iterative voting method,mobile robots,multiple real-world datasets,object detection,raw map segmentation,Real-time systems,robot vision,Silicon,Simultaneous localization and mapping,SLAM,SLAM (robots),Timing,timing complexity}
}

@article{forsterOnManifoldPreintegrationRealTime2017,
  title = {On-{{Manifold Preintegration}} for {{Real}}-{{Time Visual}}--{{Inertial Odometry}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  year = {2017},
  month = feb,
  volume = {33},
  pages = {1--21},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2016.2597321},
  abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence leading to fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a-posteriori bias correction in analytic form. The second contribution is to show that the preintegrated IMU model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modelling effort leads to accurate state estimation in real-time, outperforming state-of-the-art approaches.},
  file = {/home/akashsharma/Zotero/storage/4USPDZRZ/Forster et al. - 2017 - On-Manifold Preintegration for Real-Time Visual--I.pdf},
  journal = {IEEE Transactions on Robotics},
  language = {en},
  number = {1}
}

@inproceedings{fourieNonparametricBeliefSolution2016,
  title = {A Nonparametric Belief Solution to the {{Bayes}} Tree},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Fourie, Dehann and Leonard, John and Kaess, Michael},
  year = {2016},
  month = oct,
  pages = {2189--2196},
  publisher = {{IEEE}},
  address = {{Daejeon, South Korea}},
  doi = {10.1109/IROS.2016.7759343},
  abstract = {We relax parametric inference to a nonparametric representation towards more general solutions on factor graphs. We use the Bayes tree factorization to maximally exploit structure in the joint posterior thereby minimizing computation. We use kernel density estimation to represent a wider class of constraint beliefs, which naturally encapsulates multi-hypothesis and non-Gaussian inference. A variety of new uncertainty models can now be directly applied in the factor graph, and have the solver recover a potentially multimodal posterior. For example, data association for loop closure proposals can be incorporated at inference time without further modifications to the factor graph. Our implementation of the presented algorithm is written entirely in the Julia language, exploiting high performance parallel computing. We show a larger scale use case with the well known Victoria park mapping and localization data set inferring over uncertain loop closures.},
  file = {/home/akashsharma/Zotero/storage/UZZ4QWEE/Fourie et al. - 2016 - A nonparametric belief solution to the Bayes tree.pdf},
  isbn = {978-1-5090-3762-9},
  language = {en}
}

@article{galDropoutBayesianApproximation,
  title = {Dropout as a {{Bayesian Approximation}}:  {{Representing Model Uncertainty}} in {{Deep Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  pages = {10},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  file = {/home/akashsharma/Zotero/storage/STQPHRHH/Gal and Ghahramani - Dropout as a Bayesian Approximation  Representing.pdf},
  language = {en}
}

@article{galvez-lopezBagsBinaryWords2012,
  title = {Bags of {{Binary Words}} for {{Fast Place Recognition}} in {{Image Sequences}}},
  author = {{Galvez-L{\'o}pez}, Dorian and Tardos, Juan D.},
  year = {2012},
  month = oct,
  volume = {28},
  pages = {1188--1197},
  issn = {1941-0468},
  doi = {10.1109/TRO.2012.2197158},
  abstract = {We propose a novel method for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF features. For the first time, we build a vocabulary tree that discretizes a binary descriptor space and use the tree to speed up correspondences for geometrical verification. We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22 ms/frame in a sequence with 26 300 images that is one order of magnitude faster than previous approaches.},
  file = {/home/akashsharma/Zotero/storage/5ZGGC4F3/Galvez-López and Tardos - 2012 - Bags of Binary Words for Fast Place Recognition in.pdf;/home/akashsharma/Zotero/storage/NQFDW9PU/6202705.html},
  journal = {IEEE Transactions on Robotics},
  keywords = {accelerated segment test,Bag of binary words,bags of binary words,binary descriptor space,Cameras,computer vision,fast place recognition,FAST±BRIEF features,feature extraction,Feature extraction,geometrical verification,geometry,image sequences,Indexes,mobile robots,object recognition,place recognition,Robots,simultaneous localization and mapping,simultaneous localization and mapping (SLAM),SLAM (robots),trees (mathematics),Vectors,visual place recognition,Vocabulary,vocabulary tree},
  number = {5}
}

@article{grisettiTutorialGraphBasedSLAM,
  title = {A {{Tutorial}} on {{Graph}}-{{Based SLAM}}},
  author = {Grisetti, Giorgio and Kummerle, Rainer and Stachniss, Cyrill and Burgard, Wolfram},
  pages = {11},
  abstract = {Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements modeled by the edges. In this paper, we provide an introductory description to the graph-based SLAM problem. Furthermore, we discuss a state-of-the-art solution that is based on least-squares error minimization and exploits the structure of the SLAM problems during optimization. The goal of this tutorial is to enable the reader to implement the proposed methods from scratch.},
  file = {/home/akashsharma/Zotero/storage/KP8BQCYE/Grisetti et al. - A Tutorial on Graph-Based SLAM.pdf},
  language = {en}
}

@article{guoObjectCentricNeuralScene2020,
  title = {Object-{{Centric Neural Scene Rendering}}},
  author = {Guo, Michelle and Fathi, Alireza and Wu, Jiajun and Funkhouser, Thomas},
  year = {2020},
  month = dec,
  abstract = {We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.},
  archiveprefix = {arXiv},
  eprint = {2012.08503},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/5VZSZVPX/Guo et al. - 2020 - Object-Centric Neural Scene Rendering.pdf;/home/akashsharma/Zotero/storage/CKPZJJKN/2012.html},
  journal = {arXiv:2012.08503 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryclass = {cs}
}

@article{harleyLearningUnlabelledVideos2020,
  title = {Learning from {{Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping}}},
  author = {Harley, Adam W. and Lakshmikanth, Shrinidhi K. and Li, Fangyu and Zhou, Xian and Tung, Hsiao-Yu Fish and Fragkiadaki, Katerina},
  year = {2020},
  month = may,
  abstract = {Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D visual recognition. We propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. We show that the proposed model learns visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating the motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a scalable self-supervised task beneficial to 3D object detection.},
  archiveprefix = {arXiv},
  eprint = {1906.03764},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/AAVJ94TV/Harley et al. - 2020 - Learning from Unlabelled Videos Using Contrastive .pdf;/home/akashsharma/Zotero/storage/8CC3SB37/1906.html},
  journal = {arXiv:1906.03764 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{hartleyLeggedRobotStateEstimation2018,
  title = {Legged {{Robot State}}-{{Estimation Through Combined Forward Kinematic}} and {{Preintegrated Contact Factors}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hartley, R. and Mangelson, J. and Gan, L. and Jadidi, M. Ghaffari and Walls, J. M. and Eustice, R. M. and Grizzle, J. W.},
  year = {2018},
  month = may,
  pages = {4422--4429},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460748},
  abstract = {State-of-the-art robotic perception systems have achieved sufficiently good performance using Inertial Measurement Units (IMUs), cameras, and nonlinear optimization techniques, that they are now being deployed as technologies. However, many of these methods rely significantly on vision and often fail when visual tracking is lost due to lighting or scarcity of features. This paper presents a state-estimation technique for legged robots that takes into account the robot's kinematic model as well as its contact with the environment. We introduce forward kinematic factors and preintegrated contact factors into a factor graph framework that can be incrementally solved in real-time. The forward kinematic factor relates the robot's base pose to a contact frame through noisy encoder measurements. The preintegrated contact factor provides odometry measurements of this contact frame while accounting for possible foot slippage. Together, the two developed factors constrain the graph optimization problem allowing the robot's trajectory to be estimated. The paper evaluates the method using simulated and real sensory IMU and kinematic data from experiments with a Cassie-series robot designed by Agility Robotics. These preliminary experiments show that using the proposed method in addition to IMU decreases drift and improves localization accuracy, suggesting that its use can enable successful recovery from a loss of visual tracking.},
  file = {/home/akashsharma/Zotero/storage/J2GA6GYK/Hartley et al. - 2018 - Legged Robot State-Estimation Through Combined For.pdf;/home/akashsharma/Zotero/storage/DME23UA9/8460748.html},
  keywords = {Cameras,Foot,Kinematics,Legged locomotion,Optimization,Robot sensing systems}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/UTBVZM2T/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{heMaskRCNN2018,
  title = {Mask {{R}}-{{CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2018},
  month = jan,
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  archiveprefix = {arXiv},
  eprint = {1703.06870},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/K3F36P66/He et al. - 2018 - Mask R-CNN.pdf;/home/akashsharma/Zotero/storage/EGNKNAQS/1703.html},
  journal = {arXiv:1703.06870 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{heneinDynamicSLAMNeed2020,
  title = {Dynamic {{SLAM}}: {{The Need For Speed}}},
  shorttitle = {Dynamic {{SLAM}}},
  author = {Henein, Mina and Zhang, Jun and Mahony, Robert and Ila, Viorela},
  year = {2020},
  month = feb,
  abstract = {The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.},
  archiveprefix = {arXiv},
  eprint = {2002.08584},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/KE24L7YY/Henein et al. - 2020 - Dynamic SLAM The Need For Speed.pdf;/home/akashsharma/Zotero/storage/PZZFAPG9/2002.html},
  journal = {arXiv:2002.08584 [cs]},
  keywords = {Computer Science - Robotics},
  primaryclass = {cs}
}

@article{hornungOctoMapEfficientProbabilistic2013,
  title = {{{OctoMap}}: An Efficient Probabilistic {{3D}} Mapping Framework Based on Octrees},
  shorttitle = {{{OctoMap}}},
  author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
  year = {2013},
  month = apr,
  volume = {34},
  pages = {189--206},
  issn = {0929-5593, 1573-7527},
  doi = {10.1007/s10514-012-9321-0},
  file = {/home/akashsharma/Zotero/storage/9UCFNRLF/Hornung et al. - 2013 - OctoMap an efficient probabilistic 3D mapping fra.pdf},
  journal = {Autonomous Robots},
  language = {en},
  number = {3}
}

@inproceedings{hsiaoMHiSAM2MultihypothesisISAM2019,
  title = {{{MH}}-{{iSAM2}}: {{Multi}}-Hypothesis {{iSAM}} Using {{Bayes Tree}} and {{Hypo}}-Tree},
  shorttitle = {{{MH}}-{{iSAM2}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hsiao, Ming and Kaess, Michael},
  year = {2019},
  month = may,
  pages = {1274--1280},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICRA.2019.8793854},
  abstract = {A novel nonlinear incremental optimization algorithm MH-iSAM2 is developed to handle ambiguity in simultaneous localization and mapping (SLAM) problems in a multihypothesis fashion. It can output multiple possible solutions for each variable according to the ambiguous inputs, which is expected to greatly enhance the robustness of autonomous systems as a whole. The algorithm consists of two data structures: an extension of the original Bayes tree that allows efficient multi-hypothesis inference, and a Hypo-tree that is designed to explicitly track and associate the hypotheses of each variable as well as all the inference processes for optimization. With our proposed hypothesis pruning strategy, MH-iSAM2 enables fast optimization and avoids the exponential growth of hypotheses. We evaluate MH-iSAM2 using both simulated datasets and real-world experiments, demonstrating its improvements on the robustness and accuracy of SLAM systems.},
  file = {/home/akashsharma/Zotero/storage/8TYQKGFR/Hsiao and Kaess - 2019 - MH-iSAM2 Multi-hypothesis iSAM using Bayes Tree a.pdf},
  isbn = {978-1-5386-6027-0},
  language = {en}
}

@article{jaderbergSpatialTransformerNetworks2016,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arXiv},
  eprint = {1506.02025},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/8NUBBR74/Jaderberg et al. - 2016 - Spatial Transformer Networks.html},
  journal = {arXiv:1506.02025 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{jatavallabhulaGradSLAMDenseSLAM2019,
  title = {{{gradSLAM}}: {{Dense SLAM}} Meets {{Automatic Differentiation}}},
  shorttitle = {{{gradSLAM}}},
  author = {Jatavallabhula, Krishna Murthy and Iyer, Ganesh and Paull, Liam},
  year = {2019},
  month = oct,
  abstract = {The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Newer learning-based approaches have the potential to leverage data or task performance to directly inform the choice of representation. However, learning representations for SLAM has been an open question, because traditional SLAM systems are not end-to-end differentiable.},
  archiveprefix = {arXiv},
  eprint = {1910.10672},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/I6TIIFFZ/Jatavallabhula et al. - 2019 - gradSLAM Dense SLAM meets Automatic Differentiati.pdf},
  journal = {arXiv:1910.10672 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{kaessIncrementalSmoothingMapping,
  title = {Incremental {{Smoothing}} and {{Mapping}}},
  author = {Kaess, Michael},
  pages = {154},
  file = {/home/akashsharma/Zotero/storage/T9386BYC/Kaess - Incremental Smoothing and Mapping.pdf},
  language = {en}
}

@article{kaessISAM2IncrementalSmoothing2012,
  title = {{{iSAM2}}: {{Incremental}} Smoothing and Mapping Using the {{Bayes}} Tree},
  shorttitle = {{{iSAM2}}},
  author = {Kaess, Michael and Johannsson, Hordur and Roberts, Richard and Ila, Viorela and Leonard, John J and Dellaert, Frank},
  year = {2012},
  month = feb,
  volume = {31},
  pages = {216--235},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364911430419},
  abstract = {We present a novel data structure, the Bayes tree, that provides an algorithmic foundation enabling a better understanding of existing graphical model inference algorithms and their connection to sparse matrix factorization methods. Similar to a clique tree, a Bayes tree encodes a factored probability density, but unlike the clique tree it is directed and maps more naturally to the square root information matrix of the simultaneous localization and mapping (SLAM) problem. In this paper, we highlight three insights provided by our new data structure. First, the Bayes tree provides a better understanding of the matrix factorization in terms of probability densities. Second, we show how the fairly abstract updates to a matrix factorization translate to a simple editing of the Bayes tree and its conditional densities. Third, we apply the Bayes tree to obtain a completely novel algorithm for sparse nonlinear incremental optimization, named iSAM2, which achieves improvements in efficiency through incremental variable re-ordering and fluid relinearization, eliminating the need for periodic batch steps. We analyze various properties of iSAM2 in detail, and show on a range of real and simulated datasets that our algorithm compares favorably with other recent mapping algorithms in both quality and efficiency.},
  file = {/home/akashsharma/Zotero/storage/3I3B48GT/Kaess et al. - 2012 - iSAM2 Incremental smoothing and mapping using the.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {2}
}

@incollection{kahlerRealTimeLargeScaleDense2016,
  title = {Real-{{Time Large}}-{{Scale Dense 3D Reconstruction}} with {{Loop Closure}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {K{\"a}hler, Olaf and Prisacariu, Victor A. and Murray, David W.},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9912},
  pages = {500--516},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46484-8_30},
  abstract = {In the highly active research field of dense 3D reconstruction and modelling, loop closure is still a largely unsolved problem. While a number of previous works show how to accumulate keyframes, globally optimize their pose on closure, and compute a dense 3D model as a post-processing step, in this paper we propose an online framework which delivers a consistent 3D model to the user in real time. This is achieved by splitting the scene into submaps, and adjusting the poses of the submaps as and when required. We present a novel technique for accumulating relative pose constraints between the submaps at very little computational cost, and demonstrate how to maintain a lightweight, scalable global optimization of submap poses. In contrast to previous works, the number of submaps grows with the observed 3D scene surface, rather than with time. In addition to loop closure, the paper incorporates relocalization and provides a novel way of assessing tracking quality.},
  file = {/home/akashsharma/Zotero/storage/GS6K9XCJ/Kähler et al. - 2016 - Real-Time Large-Scale Dense 3D Reconstruction with.pdf},
  isbn = {978-3-319-46483-1 978-3-319-46484-8},
  language = {en}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/H52JCPM9/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/home/akashsharma/Zotero/storage/UJMQ3SP8/1312.html},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{kirillovPointRendImageSegmentation2020,
  title = {{{PointRend}}: {{Image Segmentation}} as {{Rendering}}},
  shorttitle = {{{PointRend}}},
  author = {Kirillov, Alexander and Wu, Yuxin and He, Kaiming and Girshick, Ross},
  year = {2020},
  month = feb,
  abstract = {We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.},
  archiveprefix = {arXiv},
  eprint = {1912.08193},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/VRJEQBFS/Kirillov et al. - 2020 - PointRend Image Segmentation as Rendering.pdf;/home/akashsharma/Zotero/storage/SESUQL5A/1912.html},
  journal = {arXiv:1912.08193 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{kleinParallelTrackingMapping2007,
  title = {Parallel {{Tracking}} and {{Mapping}} for {{Small AR Workspaces}}},
  booktitle = {2007 6th {{IEEE}} and {{ACM International Symposium}} on {{Mixed}} and {{Augmented Reality}}},
  author = {Klein, Georg and Murray, David},
  year = {2007},
  month = nov,
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Nara, Japan}},
  doi = {10.1109/ISMAR.2007.4538852},
  abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
  file = {/home/akashsharma/Zotero/storage/DE37B53L/Klein and Murray - 2007 - Parallel Tracking and Mapping for Small AR Workspa.pdf},
  isbn = {978-1-4244-1749-0},
  language = {en}
}

@inproceedings{laiUnsupervisedFeatureLearning2014,
  title = {Unsupervised Feature Learning for {{3D}} Scene Labeling},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lai, Kevin and Bo, Liefeng and Fox, Dieter},
  year = {2014},
  month = may,
  pages = {3050--3057},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907298},
  abstract = {This paper presents an approach for labeling objects in 3D scenes. We introduce HMP3D, a hierarchical sparse coding technique for learning features from 3D point cloud data. HMP3D classifiers are trained using a synthetic dataset of virtual scenes generated using CAD models from an online database. Our scene labeling system combines features learned from raw RGB-D images and 3D point clouds directly, without any hand-designed features, to assign an object label to every 3D point in the scene. Experiments on the RGB-D Scenes Dataset v.2 demonstrate that the proposed approach can be used to label indoor scenes containing both small tabletop objects and large furniture pieces.},
  file = {/home/akashsharma/Zotero/storage/M65C3GJZ/Lai et al. - 2014 - Unsupervised feature learning for 3D scene labelin.pdf;/home/akashsharma/Zotero/storage/9L22KJ78/6907298.html},
  keywords = {3D point cloud data,3D scene labeling,CAD,CAD model,Dictionaries,Feature extraction,furniture pieces,hand-designed feature,hierarchical sparse coding technique,HMP3D classifiers,image colour analysis,indoor scenes,Labeling,learning features,Matching pursuit algorithms,object label,online database,RGB-D images,RGB-D scenes dataset v.2,scene labeling system,Solid modeling,solid modelling,synthetic dataset,tabletop objects,Three-dimensional displays,unsupervised feature learning,unsupervised learning,Videos,virtual reality,virtual scenes}
}

@article{lakshminarayananSimpleScalablePredictive,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  pages = {13},
  abstract = {Deep neural networks are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in neural networks is a challenging and yet unsolved problem. Bayesian neural networks, which learn a distribution over weights, are currently the stateof-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) neural neural networks. We propose an alternative to Bayesian neural networks, that is simple to implement, readily parallelisable and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian neural networks. Finally, we evaluate the predictive uncertainty on test examples from known and unknown classes, and show that our method is able to express higher degree of uncertainty on unknown classes, unlike existing methods which make overconfident predictions even on unknown classes.},
  file = {/home/akashsharma/Zotero/storage/I9ZELJ56/Lakshminarayanan et al. - Simple and Scalable Predictive Uncertainty Estimat.pdf},
  language = {en}
}

@inproceedings{landryCELLO3DEstimatingCovariance2019,
  title = {{{CELLO}}-{{3D}}: {{Estimating}} the {{Covariance}} of {{ICP}} in the {{Real World}}},
  shorttitle = {{{CELLO}}-{{3D}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Landry, D. and Pomerleau, F. and Gigu{\`e}re, P.},
  year = {2019},
  month = may,
  pages = {8190--8196},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2019.8793516},
  abstract = {The fusion of Iterative Closest Point (ICP) registrations in existing state estimation frameworks relies on an accurate estimation of their uncertainty. In this paper, we study the estimation of this uncertainty in the form of a covariance. First, we scrutinize the limitations of existing closed-form covariance estimation algorithms over 3D datasets. Then, we set out to estimate the covariance of ICP registrations through a data-driven approach, with over 5100000 registrations on 1020 pairs from real 3D point clouds. We assess our solution upon a wide spectrum of environments, ranging from structured to unstructured and indoor to outdoor. The capacity of our algorithm to predict covariances is accurately assessed, as well as the usefulness of these estimations for uncertainty estimation over trajectories. The proposed method estimates covariances better than existing closed-form solutions, and makes predictions that are consistent with observed trajectories.},
  file = {/home/akashsharma/Zotero/storage/DYIXXD7U/Landry et al. - 2019 - CELLO-3D Estimating the Covariance of ICP in the .pdf;/home/akashsharma/Zotero/storage/ABZ8VN6C/8793516.html},
  keywords = {3D datasets,CELLO-3D,closed-form covariance estimation algorithms,closed-form solutions,Computational modeling,covariance analysis,covariance estimation and learning through likelihood optimization framework,covariance matrices,data analysis,data-driven approach,Estimation,ICP registrations,image registration,iterative closest point registrations,iterative methods,Linear programming,Measurement,Prediction algorithms,real 3D point clouds,state estimation,state estimation frameworks,Three-dimensional displays,Uncertainty,uncertainty estimation}
}

@article{linBARFBundleAdjustingNeural2021,
  title = {{{BARF}}: {{Bundle}}-{{Adjusting Neural Radiance Fields}}},
  shorttitle = {{{BARF}}},
  author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
  year = {2021},
  month = apr,
  abstract = {Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\textbackslash "ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
  archiveprefix = {arXiv},
  eprint = {2104.06405},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/Z4FAFHWX/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.html},
  journal = {arXiv:2104.06405 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{lindellAutoIntAutomaticIntegration2020,
  title = {{{AutoInt}}: {{Automatic Integration}} for {{Fast Neural Volume Rendering}}},
  shorttitle = {{{AutoInt}}},
  author = {Lindell, David B. and Martel, Julien N. P. and Wetzstein, Gordon},
  year = {2020},
  month = dec,
  abstract = {Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, implicit neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using implicit neural representation networks. For training, we instantiate the computational graph corresponding to the derivative of the implicit neural representation. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Using this approach, we demonstrate a greater than 10x improvement in computation requirements, enabling fast neural volume rendering.},
  archiveprefix = {arXiv},
  eprint = {2012.01714},
  eprinttype = {arxiv},
  journal = {arXiv:2012.01714 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{liuNeuralSparseVoxel2020,
  title = {Neural {{Sparse Voxel Fields}}},
  author = {Liu, Lingjie and Gu, Jiatao and Lin, Kyaw Zaw and Chua, Tat-Seng and Theobalt, Christian},
  year = {2020},
  month = jul,
  abstract = {Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a diffentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is over 10 times faster than the state-of-the-art (namely, NeRF) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering.},
  archiveprefix = {arXiv},
  eprint = {2007.11571},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/UZGRN2SE/Liu et al. - 2020 - Neural Sparse Voxel Fields.pdf;/home/akashsharma/Zotero/storage/TCEDJYZ8/2007.html},
  journal = {arXiv:2007.11571 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{liuSemanticCorrespondenceOptimal,
  title = {Semantic {{Correspondence}} as an {{Optimal Transport Problem}}},
  author = {Liu, Yanbin and Zhu, Linchao and Yamada, Makoto and Yang, Yi},
  pages = {10},
  abstract = {Establishing dense correspondences across semantically similar images is a challenging task. Due to the large intra-class variation and background clutter, two common issues occur in current approaches. First, many pixels in a source image are assigned to one target pixel, i.e., many to one matching. Second, some object pixels are assigned to the background pixels, i.e., background matching. We solve the first issue by global feature matching, which maximizes the total matching correlations between images to obtain a global optimal matching matrix. The row sum and column sum constraints are enforced on the matching matrix to induce a balanced solution, thus suppressing many to one matching. We solve the second issue by applying a staircase function on the class activation maps to re-weight the importance of pixels into four levels from foreground to background. The whole procedure is combined into a unified optimal transport algorithm by converting the maximization problem to the optimal transport formulation and incorporating the staircase weights into optimal transport algorithm to act as empirical distributions. The proposed algorithm achieves state-of-the-art performance on four benchmark datasets. Notably, a 26\% relative improvement is achieved on the large-scale SPair-71k dataset.},
  file = {/home/akashsharma/Zotero/storage/MCEP2T4Y/Liu et al. - Semantic Correspondence as an Optimal Transport Pr.pdf},
  language = {en}
}

@article{mccormacFusionVolumetricObjectLevel2018,
  title = {Fusion++: {{Volumetric Object}}-{{Level SLAM}}},
  shorttitle = {Fusion++},
  author = {McCormac, John and Clark, Ronald and Bloesch, Michael and Davison, Andrew J. and Leutenegger, Stefan},
  year = {2018},
  month = aug,
  abstract = {We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally refined via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is refined over time and an existence probability to account for spurious instance predictions. We demonstrate our approach on a hand-held RGB-D sequence from a cluttered office scene with a large number and variety of object instances, highlighting how the system closes loops and makes good use of existing objects on repeated loops. We quantitatively evaluate the trajectory error of our system against a baseline approach on the RGB-D SLAM benchmark, and qualitatively compare reconstruction quality of discovered objects on the YCB video dataset. Performance evaluation shows our approach is highly memory efficient and runs online at 4-8Hz (excluding relocalisation) despite not being optimised at the software level.},
  archiveprefix = {arXiv},
  eprint = {1808.08378},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/5AZEXCX5/McCormac et al. - 2018 - Fusion++ Volumetric Object-Level SLAM.pdf;/home/akashsharma/Zotero/storage/XU6A3T5H/1808.html},
  journal = {arXiv:1808.08378 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@incollection{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12346},
  pages = {405--421},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58452-8_24},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (\texttheta, {$\varphi$})) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  file = {/home/akashsharma/Downloads/2003.08934.pdf;/home/akashsharma/Zotero/storage/9J6SWAB2/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf},
  isbn = {978-3-030-58451-1 978-3-030-58452-8},
  language = {en}
}

@article{millaneCbloxScalableConsistent2018,
  title = {C-Blox: {{A Scalable}} and {{Consistent TSDF}}-Based {{Dense Mapping Approach}}},
  shorttitle = {C-Blox},
  author = {Millane, Alexander and Taylor, Zachary and Oleynikova, Helen and Nieto, Juan and Siegwart, Roland and Cadena, C{\'e}sar},
  year = {2018},
  month = sep,
  abstract = {In many applications, maintaining a consistent dense map of the environment is key to enabling robotic platforms to perform higher level decision making. Several works have addressed the challenge of creating precise dense 3D maps from visual sensors providing depth information. However, during operation over longer missions, reconstructions can easily become inconsistent due to accumulated camera tracking error and delayed loop closure. Without explicitly addressing the problem of map consistency, recovery from such distortions tends to be difficult. We present a novel system for dense 3D mapping which addresses the challenge of building consistent maps while dealing with scalability. Central to our approach is the representation of the environment as a collection of overlapping TSDF subvolumes. These subvolumes are localized through feature-based camera tracking and bundle adjustment. Our main contribution is a pipeline for identifying stable regions in the map, and to fuse the contributing subvolumes. This approach allows us to reduce map growth while still maintaining consistency. We demonstrate the proposed system on a publicly available dataset and simulation engine, and demonstrate the efficacy of the proposed approach for building consistent and scalable maps. Finally we demonstrate our approach running in real-time on-board a lightweight MAV.},
  archiveprefix = {arXiv},
  eprint = {1710.07242},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/66EYIIPC/Millane et al. - 2018 - C-blox A Scalable and Consistent TSDF-based Dense.pdf;/home/akashsharma/Zotero/storage/B3CP2W2W/1710.html},
  journal = {arXiv:1710.07242 [cs]},
  keywords = {Computer Science - Robotics},
  primaryclass = {cs}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2017,
  title = {{{ORB}}-{{SLAM2}}: {{An Open}}-{{Source SLAM System}} for {{Monocular}}, {{Stereo}}, and {{RGB}}-{{D Cameras}}},
  shorttitle = {{{ORB}}-{{SLAM2}}},
  author = {{Mur-Artal}, R. and Tard{\'o}s, J. D.},
  year = {2017},
  month = oct,
  volume = {33},
  pages = {1255--1262},
  issn = {1941-0468},
  doi = {10.1109/TRO.2017.2705103},
  abstract = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  file = {/home/akashsharma/Zotero/storage/7MNN69LT/Mur-Artal and Tardós - 2017 - ORB-SLAM2 An Open-Source SLAM System for Monocula.pdf;/home/akashsharma/Zotero/storage/CWC9JJLE/7946260.html},
  journal = {IEEE Transactions on Robotics},
  keywords = {cameras,Cameras,distance measurement,Feature extraction,Kalman filters,lightweight localization mode,Localization,map points,mapping,mobile robots,monocular cameras,motion estimation,open-source SLAM system,Optimization,ORB-SLAM,path planning,RGB-D,RGB-D cameras,robot vision,Simultaneous localization and mapping,simultaneous localization and mapping (SLAM),simultaneous localization and mapping system,SLAM (robots),SLAM community,stereo,stereo cameras,Tracking loops,Trajectory,zero-drift localization},
  number = {5}
}

@article{mur-artalORBSLAMVersatileAccurate2015,
  title = {{{ORB}}-{{SLAM}}: {{A Versatile}} and {{Accurate Monocular SLAM System}}},
  shorttitle = {{{ORB}}-{{SLAM}}},
  author = {{Mur-Artal}, Raul and Montiel, J. M. M. and Tardos, Juan D.},
  year = {2015},
  month = oct,
  volume = {31},
  pages = {1147--1163},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2015.2463671},
  abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORBSLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  file = {/home/akashsharma/Zotero/storage/97LBUI86/Mur-Artal et al. - 2015 - ORB-SLAM A Versatile and Accurate Monocular SLAM .pdf},
  journal = {IEEE Transactions on Robotics},
  language = {en},
  number = {5}
}

@book{murrayMathematicalIntroductionRobotic1994,
  title = {A {{Mathematical Introduction}} to {{Robotic Manipulation}}},
  author = {Murray, Richard M. and Li, Zexiang and Sastry, S. Shankar},
  year = {1994},
  file = {/home/akashsharma/Zotero/storage/NST37FXK/Murray et al. - 1994 - A Mathematical Introduction to Robotic Manipulatio.pdf;/home/akashsharma/Zotero/storage/8MGCE3YB/summary.html}
}

@inproceedings{NEURIPS2019_475fbefa,
  title = {Fast and Accurate Least-Mean-Squares Solvers},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Maalouf, Alaa and Jubran, Ibrahim and Feldman, Dan},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  volume = {32},
  pages = {8307--8318},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{newcombeKinectFusionRealtimeDense2011,
  title = {{{KinectFusion}}: {{Real}}-Time Dense Surface Mapping and Tracking},
  shorttitle = {{{KinectFusion}}},
  booktitle = {2011 10th {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}}},
  author = {Newcombe, R. A. and Izadi, S. and Hilliges, O. and Molyneaux, D. and Kim, D. and Davison, A. J. and Kohi, P. and Shotton, J. and Hodges, S. and Fitzgibbon, A.},
  year = {2011},
  month = oct,
  pages = {127--136},
  doi = {10.1109/ISMAR.2011.6092378},
  abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  file = {/home/akashsharma/Zotero/storage/Z9GQZ88S/Newcombe et al. - 2011 - KinectFusion Real-time dense surface mapping and .pdf},
  keywords = {AR,Cameras,Dense Reconstruction,Depth Cameras,GPU,Image reconstruction,Iterative closest point algorithm,Real time systems,Real-Time,Simultaneous localization and mapping,SLAM,Surface reconstruction,Three dimensional displays,Tracking,Volumetric Representation}
}

@article{nicholsonQuadricSLAMDualQuadrics2019,
  title = {{{QuadricSLAM}}: {{Dual Quadrics From Object Detections}} as {{Landmarks}} in {{Object}}-{{Oriented SLAM}}},
  shorttitle = {{{QuadricSLAM}}},
  author = {Nicholson, Lachlan and Milford, Michael and S{\"u}nderhauf, Niko},
  year = {2019},
  month = jan,
  volume = {4},
  pages = {1--8},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2866205},
  abstract = {In this letter, we use two-dimensional (2-D) object detections from multiple views to simultaneously estimate a 3-D quadric surface for each object and localize the camera position. We derive a simultaneous localization and mapping (SLAM) formulation that uses dual quadrics as 3-D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2-D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.},
  file = {/home/akashsharma/Zotero/storage/RS5I84TH/Nicholson et al. - 2019 - QuadricSLAM Dual Quadrics From Object Detections .pdf;/home/akashsharma/Zotero/storage/5XYTF35W/8440105.html},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {2-D object detections,3-D landmark representations,3-D quadric surface,camera position,cameras,Cameras,Detectors,dual quadric parameters,Ellipsoids,factor graph based SLAM,general perspective camera,geometric error formulation,graph theory,mobile robots,object detection,Object detection,object detectors,object-oriented SLAM,partially visible objects,robot vision,semantic scene understanding,Semantics,simultaneous localization and mapping,Simultaneous localization and mapping,SLAM,SLAM (robots),SLAM formulation,Three-dimensional displays},
  number = {1}
}

@article{niessnerRealtime3DReconstruction2013,
  title = {Real-Time {{3D}} Reconstruction at Scale Using Voxel Hashing},
  author = {Nie{\ss}ner, Matthias and Zollh{\"o}fer, Michael and Izadi, Shahram and Stamminger, Marc},
  year = {2013},
  month = nov,
  volume = {32},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/2508363.2508374},
  abstract = {Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.},
  file = {/home/akashsharma/Zotero/storage/IRMYJ8UU/Nießner et al. - 2013 - Real-time 3D reconstruction at scale using voxel h.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@article{ostNeuralSceneGraphs2020,
  title = {Neural {{Scene Graphs}} for {{Dynamic Scenes}}},
  author = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
  year = {2020},
  month = nov,
  abstract = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient interpolations of static scenes that encode all scene objects into a single neural network, lacking the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes - only by observing a video of this scene - and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
  archiveprefix = {arXiv},
  eprint = {2011.10379},
  eprinttype = {arxiv},
  journal = {arXiv:2011.10379 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryclass = {cs}
}

@inproceedings{parkColoredPointCloud2017,
  title = {Colored {{Point Cloud Registration Revisited}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2017},
  month = oct,
  pages = {143--152},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.25},
  abstract = {We present an algorithm for aligning two colored point clouds. The key idea is to optimize a joint photometric and geometric objective that locks the alignment along both the normal direction and the tangent plane. We extend a photometric objective for aligning RGB-D images to point clouds, by locally parameterizing the point cloud with a virtual camera. Experiments demonstrate that our algorithm is more accurate and more robust than prior point cloud registration algorithms, including those that utilize color information. We use the presented algorithms to enhance a state-of-the-art scene reconstruction system. The precision of the resulting system is demonstrated on real-world scenes with accurate ground-truth models.},
  file = {/home/akashsharma/Zotero/storage/JJZIGKJZ/Park et al. - 2017 - Colored Point Cloud Registration Revisited.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@inproceedings{parkDeepSDFLearningContinuous2019,
  title = {{{DeepSDF}}: {{Learning Continuous Signed Distance Functions}} for {{Shape Representation}}},
  shorttitle = {{{DeepSDF}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  year = {2019},
  pages = {165--174},
  file = {/home/akashsharma/Zotero/storage/2AWRFB33/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf;/home/akashsharma/Zotero/storage/6EPUQFQ7/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_p.html}
}

@book{peyreComputationalOptimalTransport2019,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2019},
  publisher = {{now Publishers Inc}},
  doi = {10.1561/9781680835519},
  file = {/home/akashsharma/Zotero/storage/DN6IFC4P/Peyré and Cuturi - 2019 - Computational Optimal Transport.pdf},
  isbn = {978-1-68083-551-9},
  language = {en}
}

@article{prisacariuInfiniTAMV3Framework2017,
  title = {{{InfiniTAM}} v3: {{A Framework}} for {{Large}}-{{Scale 3D Reconstruction}} with {{Loop Closure}}},
  shorttitle = {{{InfiniTAM}} V3},
  author = {Prisacariu, Victor Adrian and K{\"a}hler, Olaf and Golodetz, Stuart and Sapienza, Michael and Cavallari, Tommaso and Torr, Philip H. S. and Murray, David W.},
  year = {2017},
  month = aug,
  abstract = {Volumetric models have become a popular representation for 3D scenes in recent years. One breakthrough leading to their popularity was KinectFusion, which focuses on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a TSDF leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is memory-intensive and limits applicability to small-scale reconstructions. Several avenues have been explored to overcome this. With the aim of summarizing them and providing for a fast, flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The idea is that steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the user's needs. This report describes the technical implementation details of InfiniTAM v3, the third version of our InfiniTAM system. We have added various new features, as well as making numerous enhancements to the low-level code that significantly improve our camera tracking performance. The new features that we expect to be of most interest are (i) a robust camera tracking module; (ii) an implementation of Glocker et al.'s keyframe-based random ferns camera relocaliser; (iii) a novel approach to globally-consistent TSDF-based reconstruction, based on dividing the scene into rigid submaps and optimising the relative poses between them; and (iv) an implementation of Keller et al.'s surfel-based reconstruction approach.},
  archiveprefix = {arXiv},
  eprint = {1708.00783},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/7N93CPJJ/Prisacariu et al. - 2017 - InfiniTAM v3 A Framework for Large-Scale 3D Recon.pdf;/home/akashsharma/Zotero/storage/CBY2YBJN/1708.html},
  journal = {arXiv:1708.00783 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{quenzelPhotometricConsistencyGradientbased2020,
  title = {Beyond {{Photometric Consistency}}: {{Gradient}}-Based {{Dissimilarity}} for {{Improving Visual Odometry}} and {{Stereo Matching}}},
  shorttitle = {Beyond {{Photometric Consistency}}},
  author = {Quenzel, Jan and Rosu, Radu Alexandru and L{\"a}be, Thomas and Stachniss, Cyrill and Behnke, Sven},
  year = {2020},
  month = apr,
  abstract = {Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicats that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.},
  archiveprefix = {arXiv},
  eprint = {2004.04090},
  eprinttype = {arxiv},
  journal = {arXiv:2004.04090 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{rabinerTutorialHiddenMarkov1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  year = {1989},
  month = feb,
  volume = {77},
  pages = {257--286},
  issn = {1558-2256},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{$<>$}},
  file = {/home/akashsharma/Zotero/storage/PXIQE3MD/Rabiner - 1989 - A tutorial on hidden Markov models and selected ap.pdf;/home/akashsharma/Zotero/storage/WFUXN5Z4/18626.html},
  journal = {Proceedings of the IEEE},
  keywords = {balls-in-urns system,coin-tossing,discrete Markov chains,ergodic models,hidden Markov models,Hidden Markov models,hidden states,left-right models,Markov processes,probabilistic function,speech recognition,Speech recognition,Tutorial},
  number = {2}
}

@article{renFasterRCNNRealTime2016,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R}}-{{CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  month = jan,
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  eprint = {1506.01497},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/DA8R32S9/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf;/home/akashsharma/Zotero/storage/HZZ4P2TW/1506.html},
  journal = {arXiv:1506.01497 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{rosinol3DDynamicScene2020,
  title = {{{3D Dynamic Scene Graphs}}: {{Actionable Spatial Perception}} with {{Places}}, {{Objects}}, and {{Humans}}},
  shorttitle = {{{3D Dynamic Scene Graphs}}},
  author = {Rosinol, Antoni and Gupta, Arjun and Abate, Marcus and Shi, Jingnan and Carlone, Luca},
  year = {2020},
  month = feb,
  abstract = {We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction. A video abstract is available at https://youtu.be/SWbofjhyPzI},
  archiveprefix = {arXiv},
  eprint = {2002.06289},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/JNHVALEY/Rosinol et al. - 2020 - 3D Dynamic Scene Graphs Actionable Spatial Percep.pdf;/home/akashsharma/Zotero/storage/HZB9DVX9/2002.html},
  journal = {arXiv:2002.06289 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{rosinolKimeraOpenSourceLibrary2020,
  title = {Kimera: An {{Open}}-{{Source Library}} for {{Real}}-{{Time Metric}}-{{Semantic Localization}} and {{Mapping}}},
  shorttitle = {Kimera},
  author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
  year = {2020},
  month = mar,
  abstract = {We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
  archiveprefix = {arXiv},
  eprint = {1910.02490},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/S6VF9HXC/Rosinol et al. - 2020 - Kimera an Open-Source Library for Real-Time Metri.pdf;/home/akashsharma/Zotero/storage/669FVCS2/1910.html},
  journal = {arXiv:1910.02490 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{runzMaskFusionRealTimeRecognition2018,
  title = {{{MaskFusion}}: {{Real}}-{{Time Recognition}}, {{Tracking}} and {{Reconstruction}} of {{Multiple Moving Objects}}},
  shorttitle = {{{MaskFusion}}},
  author = {R{\"u}nz, Martin and Buffier, Maud and Agapito, Lourdes},
  year = {2018},
  month = apr,
  abstract = {We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera.   As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable real-time object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic.},
  file = {/home/akashsharma/Zotero/storage/ASAEGR89/Rünz et al. - 2018 - MaskFusion Real-Time Recognition, Tracking and Re.pdf;/home/akashsharma/Zotero/storage/J8ECIRDW/1804.html},
  language = {en}
}

@inproceedings{rusinkiewiczEfficientVariantsICP2001,
  title = {Efficient Variants of the {{ICP}} Algorithm},
  booktitle = {Proceedings {{Third International Conference}} on 3-{{D Digital Imaging}} and {{Modeling}}},
  author = {Rusinkiewicz, S. and Levoy, M.},
  year = {2001},
  pages = {145--152},
  publisher = {{IEEE Comput. Soc}},
  address = {{Quebec City, Que., Canada}},
  doi = {10.1109/IM.2001.924423},
  abstract = {The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.},
  file = {/home/akashsharma/Zotero/storage/8GQ3RDQX/Rusinkiewicz and Levoy - 2001 - Efficient variants of the ICP algorithm.pdf},
  isbn = {978-0-7695-0984-6},
  language = {en}
}

@inproceedings{rusuFastPointFeature2009,
  title = {Fast {{Point Feature Histograms}} ({{FPFH}}) for {{3D}} Registration},
  booktitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Rusu, Radu Bogdan and Blodow, Nico and Beetz, Michael},
  year = {2009},
  month = may,
  pages = {3212--3217},
  publisher = {{IEEE}},
  address = {{Kobe}},
  doi = {10.1109/ROBOT.2009.5152473},
  abstract = {In our recent work [1], [2], we proposed Point Feature Histograms (PFH) as robust multi-dimensional features which describe the local geometry around a point p for 3D point cloud datasets. In this paper, we modify their mathematical expressions and perform a rigorous analysis on their robustness and complexity for the problem of 3D registration for overlapping point cloud views. More concretely, we present several optimizations that reduce their computation times drastically by either caching previously computed values or by revising their theoretical formulations. The latter results in a new type of local features, called Fast Point Feature Histograms (FPFH), which retain most of the discriminative power of the PFH. Moreover, we propose an algorithm for the online computation of FPFH features for realtime applications. To validate our results we demonstrate their efficiency for 3D registration and propose a new sample consensus based method for bringing two datasets into the convergence basin of a local non-linear optimizer: SAC-IA (SAmple Consensus Initial Alignment).},
  file = {/home/akashsharma/Zotero/storage/2ESRG7CN/Rusu et al. - 2009 - Fast Point Feature Histograms (FPFH) for 3D regist.pdf},
  isbn = {978-1-4244-2788-8},
  language = {en}
}

@inproceedings{salas-morenoSLAMSimultaneousLocalisation2013,
  title = {{{SLAM}}++: {{Simultaneous Localisation}} and {{Mapping}} at the {{Level}} of {{Objects}}},
  shorttitle = {{{SLAM}}++},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Salas-Moreno}, Renato F. and Newcombe, Richard A. and Strasdat, Hauke and Kelly, Paul H.J. and Davison, Andrew J.},
  year = {2013},
  month = jun,
  pages = {1352--1359},
  publisher = {{IEEE}},
  address = {{Portland, OR, USA}},
  doi = {10.1109/CVPR.2013.178},
  abstract = {We present the major advantages of a new `object oriented' 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, realtime 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph optimisation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction, but with a huge representation compression. The object graph enables predictions for accurate ICP-based camera to model tracking at each live frame, and efficient active search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relocalisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.},
  file = {/home/akashsharma/Zotero/storage/3ELHVJ4K/Salas-Moreno et al. - 2013 - SLAM++ Simultaneous Localisation and Mapping at t.pdf},
  isbn = {978-0-7695-4989-7},
  language = {en}
}

@article{sarlinSuperGlueLearningFeature2020,
  title = {{{SuperGlue}}: {{Learning Feature Matching}} with {{Graph Neural Networks}}},
  shorttitle = {{{SuperGlue}}},
  author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.},
  archiveprefix = {arXiv},
  eprint = {1911.11763},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/XH5D45DP/Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Ne.pdf;/home/akashsharma/Zotero/storage/4A27ARJL/1911.html},
  journal = {arXiv:1911.11763 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{shanLIOSAMTightlycoupledLidar2020,
  title = {{{LIO}}-{{SAM}}: {{Tightly}}-Coupled Lidar Inertial Odometry via Smoothing and Mapping},
  booktitle = {{{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  author = {Shan, Tixiao and Englot, Brendan and Meyers, Drew and Wang, Wei and Ratti, Carlo and Daniela, Rus},
  year = {2020},
  pages = {5135--5142},
  organization = {{IEEE}}
}

@article{sharmaCompositionalScalableObject2020,
  title = {Compositional {{Scalable Object SLAM}}},
  author = {Sharma, Akash and Dong, Wei and Kaess, Michael},
  year = {2020},
  month = nov,
  abstract = {We present a fast, scalable, and accurate Simultaneous Localization and Mapping (SLAM) system that represents indoor scenes as a graph of objects. Leveraging the observation that artificial environments are structured and occupied by recognizable objects, we show that a compositional scalable object mapping formulation is amenable to a robust SLAM solution for drift-free large scale indoor reconstruction. To achieve this, we propose a novel semantically assisted data association strategy that obtains unambiguous persistent object landmarks, and a 2.5D compositional rendering method that enables reliable frame-to-model RGB-D tracking. Consequently, we deliver an optimized online implementation that can run at near frame rate with a single graphics card, and provide a comprehensive evaluation against state of the art baselines. An open source implementation will be provided at https://placeholder.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {2011.02658},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/3ES5PET4/Sharma et al. - 2020 - Compositional Scalable Object SLAM.pdf;/home/akashsharma/Zotero/storage/9PDE58NB/2011.html},
  journal = {arXiv:2011.02658 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{sitzmannSELFSUPERVISEDSCENEREPRESENTATION,
  title = {{{SELF}}-{{SUPERVISED SCENE REPRESENTATION LEARNING}}},
  author = {Sitzmann, Vincent},
  pages = {97},
  file = {/home/akashsharma/Zotero/storage/YZZ97BML/Sitzmann - SELF-SUPERVISED SCENE REPRESENTATION LEARNING.pdf},
  language = {en}
}

@article{solaMicroLieTheory2020,
  title = {A Micro {{Lie}} Theory for State Estimation in Robotics},
  author = {Sol{\`a}, Joan and Deray, Jeremie and Atchuthan, Dinesh},
  year = {2020},
  month = aug,
  abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
  archiveprefix = {arXiv},
  eprint = {1812.01537},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/I5BAD89B/Solà et al. - 2020 - A micro Lie theory for state estimation in robotic.pdf;/home/akashsharma/Zotero/storage/SV5RSV97/1812.html},
  journal = {arXiv:1812.01537 [cs]},
  keywords = {Computer Science - Robotics},
  primaryclass = {cs}
}

@article{solaQuaternionKinematicsErrorstate2017,
  title = {Quaternion Kinematics for the Error-State {{Kalman}} Filter},
  author = {Sol{\`a}, Joan},
  year = {2017},
  month = nov,
  abstract = {This article is an exhaustive revision of concepts and formulas related to quaternions and rotations in 3D space, and their proper use in estimation engines such as the error-state Kalman filter. The paper includes an in-depth study of the rotation group and its Lie structure, with formulations using both quaternions and rotation matrices. It makes special attention in the definition of rotation perturbations, derivatives and integrals. It provides numerous intuitions and geometrical interpretations to help the reader grasp the inner mechanisms of 3D rotation. The whole material is used to devise precise formulations for error-state Kalman filters suited for real applications using integration of signals from an inertial measurement unit (IMU).},
  archiveprefix = {arXiv},
  eprint = {1711.02508},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/RL7FYICV/Solà - 2017 - Quaternion kinematics for the error-state Kalman f.pdf;/home/akashsharma/Zotero/storage/GNTPAHNG/1711.html},
  journal = {arXiv:1711.02508 [cs]},
  keywords = {Computer Science - Robotics},
  primaryclass = {cs}
}

@article{solomonConvolutionalWassersteinDistances,
  title = {Convolutional {{Wasserstein Distances}}: {{Efficient Optimal Transportation}} on {{Geometric Domains}}},
  author = {Solomon, Justin and {de Goes}, Fernando and Peyre, Gabriel and {Paris-Dauphine}, Univ and Cuturi, Marco},
  pages = {11},
  abstract = {This paper introduces a new class of algorithms for optimization problems involving optimal transportation over geometric domains. Our main contribution is to show that optimal transportation can be made tractable over large domains used in graphics, such as images and triangle meshes, improving performance by orders of magnitude compared to previous work. To this end, we approximate optimal transportation distances using entropic regularization. The resulting objective contains a geodesic distance-based kernel that can be approximated with the heat kernel. This approach leads to simple iterative numerical schemes with linear convergence, in which each iteration only requires Gaussian convolution or the solution of a sparse, pre-factored linear system. We demonstrate the versatility and efficiency of our method on tasks including reflectance interpolation, color transfer, and geometry processing.},
  file = {/home/akashsharma/Zotero/storage/9J55WUHV/Solomon et al. - Convolutional Wasserstein Distances Efﬁcient Opti.pdf},
  language = {en}
}

@article{straubReplicaDatasetDigital2019,
  title = {The {{Replica Dataset}}: {{A Digital Replica}} of {{Indoor Spaces}}},
  shorttitle = {The {{Replica Dataset}}},
  author = {Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan and Wijmans, Erik and Green, Simon and Engel, Jakob J. and {Mur-Artal}, Raul and Ren, Carl and Verma, Shobhit and Clarkson, Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan, Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and Carter, Nigel and Briales, Jesus and Gillingham, Tyler and Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra, Dhruv and Strasdat, Hauke M. and De Nardi, Renzo and Goesele, Michael and Lovegrove, Steven and Newcombe, Richard},
  year = {2019},
  month = jun,
  abstract = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  archiveprefix = {arXiv},
  eprint = {1906.05797},
  eprinttype = {arxiv},
  journal = {arXiv:1906.05797 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryclass = {cs, eess}
}

@inproceedings{sturmBenchmarkEvaluationRGBD2012,
  title = {A Benchmark for the Evaluation of {{RGB}}-{{D SLAM}} Systems},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  year = {2012},
  month = oct,
  pages = {573--580},
  publisher = {{IEEE}},
  address = {{Vilamoura-Algarve, Portugal}},
  doi = {10.1109/IROS.2012.6385773},
  abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 \texttimes{} 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
  file = {/home/akashsharma/Zotero/storage/QB85WDKR/Sturm et al. - 2012 - A benchmark for the evaluation of RGB-D SLAM syste.pdf},
  isbn = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1},
  language = {en}
}

@article{sumikuraOpenVSLAMVersatileVisual2019,
  title = {{{OpenVSLAM}}: {{A Versatile Visual SLAM Framework}}},
  shorttitle = {{{OpenVSLAM}}},
  author = {Sumikura, Shinya and Shibuya, Mikiya and Sakurada, Ken},
  year = {2019},
  month = oct,
  pages = {2292--2295},
  doi = {10.1145/3343031.3350539},
  abstract = {In this paper, we introduce OpenVSLAM, a visual SLAM framework with high usability and extensibility. Visual SLAM systems are essential for AR devices, autonomous control of robots and drones, etc. However, conventional open-source visual SLAM frameworks are not appropriately designed as libraries called from third-party programs. To overcome this situation, we have developed a novel visual SLAM framework. This software is designed to be easily used and extended. It incorporates several useful features and functions for research and development. OpenVSLAM is released at https://github.com/xdspacelab/openvslam under the 2-clause BSD license.},
  archiveprefix = {arXiv},
  eprint = {1910.01122},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/GB9TIXEM/Sumikura et al. - 2019 - OpenVSLAM A Versatile Visual SLAM Framework.pdf;/home/akashsharma/Zotero/storage/8VURWG32/1910.html},
  journal = {Proceedings of the 27th ACM International Conference on Multimedia},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}

@inproceedings{sunderhaufSwitchableConstraintsRobust2012,
  title = {Switchable {{Constraints}} for {{Robust Pose Graph SLAM}}},
  booktitle = {In {{Proc}}. of {{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}},
  author = {S{\"u}nderhauf, Niko and Protzel, Peter},
  year = {2012},
  abstract = {c{$\bigcirc$}2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating},
  file = {/home/akashsharma/Zotero/storage/9297592Y/Sünderhauf and Protzel - 2012 - Switchable Constraints for Robust Pose Graph SLAM.pdf;/home/akashsharma/Zotero/storage/ZJ7M5ATT/summary.html}
}

@inproceedings{sungjoonchoiRobustReconstructionIndoor2015,
  title = {Robust Reconstruction of Indoor Scenes},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Sungjoon Choi} and Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2015},
  month = jun,
  pages = {5556--5565},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7299195},
  abstract = {We present an approach to indoor scene reconstruction from RGB-D video. The key idea is to combine geometric registration of scene fragments with robust global optimization based on line processes. Geometric registration is error-prone due to sensor noise, which leads to aliasing of geometric detail and inability to disambiguate different surfaces in the scene. The presented optimization approach disables erroneous geometric alignments even when they significantly outnumber correct ones. Experimental results demonstrate that the presented approach substantially increases the accuracy of reconstructed scene models.},
  file = {/home/akashsharma/Zotero/storage/USNYK27J/Sungjoon Choi et al. - 2015 - Robust reconstruction of indoor scenes.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@article{tancikFourierFeaturesLet2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  year = {2020},
  month = jun,
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  archiveprefix = {arXiv},
  eprint = {2006.10739},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/KML7DAK7/Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency.pdf;/home/akashsharma/Zotero/storage/IQ4FWWHA/2006.html},
  journal = {arXiv:2006.10739 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{tangLSMLearningSubspace,
  title = {{{LSM}}: {{Learning Subspace Minimization}} for {{Low}}-{{Level Vision}}},
  author = {Tang, Chengzhou and Yuan, Lu and Tan, Ping},
  pages = {12},
  abstract = {We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principle of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with completely shared parameters, and even generalizes the trained network to an unseen task as long as its data term can be formulated. We demonstrate our LSM framework on four low-level tasks including interactive image segmentation, video segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments show that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference.},
  file = {/home/akashsharma/Zotero/storage/UE32BFRT/Tang et al. - LSM Learning Subspace Minimization for Low-Level .pdf},
  language = {en}
}

@inproceedings{tatenoRealtimeScalableIncremental2015,
  title = {Real-Time and Scalable Incremental Segmentation on Dense {{SLAM}}},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Tateno, Keisuke and Tombari, Federico and Navab, Nassir},
  year = {2015},
  month = sep,
  pages = {4465--4472},
  doi = {10.1109/IROS.2015.7354011},
  abstract = {This work proposes a real-time segmentation method for 3D point clouds obtained via Simultaneous Localization And Mapping (SLAM). The proposed method incrementally merges segments obtained from each input depth image in a unified global model using a SLAM framework. Differently from all other approaches, our method is able to yield segmentation of scenes reconstructed from multiple views in real-time, with a complexity that does not depend on the size of the global model. At the same time, it is also general, as it can be deployed with any frame-wise segmentation approach as well as any SLAM algorithm. We validate our proposal by a comparison with the state of the art in terms of computational efficiency and accuracy on a benchmark dataset, as well as by showing how our method can enable real-time segmentation from reconstructions of diverse real indoor environments.},
  file = {/home/akashsharma/Zotero/storage/DCUC2VGK/Tateno et al. - 2015 - Real-time and scalable incremental segmentation on.pdf;/home/akashsharma/Zotero/storage/ZDF8JYQQ/7354011.html},
  keywords = {3D point clouds,benchmark dataset,Cameras,computational accuracy analysis,computational efficiency analysis,dense SLAM,edge detection,frame-wise segmentation approach,global model,GSM,image reconstruction,image segmentation,Image segmentation,incremental segment merging,input depth image,Merging,real indoor environments,real-time scalable incremental segmentation,Real-time systems,robot vision,scene reconstruction,Simultaneous localization and mapping,simultaneous localization-and-mapping,SLAM (robots),Three-dimensional displays,unified global model}
}

@article{teedDeepV2DVideoDepth2020,
  title = {{{DeepV2D}}: {{Video}} to {{Depth}} with {{Differentiable Structure}} from {{Motion}}},
  shorttitle = {{{DeepV2D}}},
  author = {Teed, Zachary and Deng, Jia},
  year = {2020},
  month = apr,
  abstract = {We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video. DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. Code is available https://github.com/princeton-vl/DeepV2D.},
  archiveprefix = {arXiv},
  eprint = {1812.04605},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/3PCUA9XS/Teed and Deng - 2020 - DeepV2D Video to Depth with Differentiable Struct.pdf;/home/akashsharma/Zotero/storage/YFEVPEXQ/1812.html},
  journal = {arXiv:1812.04605 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{tewariStateArtNeural2020,
  title = {State of the {{Art}} on {{Neural Rendering}}},
  author = {Tewari, Ayush and Fried, Ohad and Thies, Justus and Sitzmann, Vincent and Lombardi, Stephen and Sunkavalli, Kalyan and {Martin-Brualla}, Ricardo and Simon, Tomas and Saragih, Jason and Nie{\ss}ner, Matthias and Pandey, Rohit and Fanello, Sean and Wetzstein, Gordon and Zhu, Jun-Yan and Theobalt, Christian and Agrawala, Maneesh and Shechtman, Eli and Goldman, Dan B. and Zollh{\"o}fer, Michael},
  year = {2020},
  month = apr,
  abstract = {Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.},
  archiveprefix = {arXiv},
  eprint = {2004.03805},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/ERBE7H6Y/Tewari et al. - 2020 - State of the Art on Neural Rendering.pdf},
  journal = {arXiv:2004.03805 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  language = {en},
  primaryclass = {cs}
}

@book{thrunProbabilisticRoboticsIntelligent2005,
  title = {Probabilistic {{Robotics}} ({{Intelligent Robotics}} and {{Autonomous Agents}})},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year = {2005},
  publisher = {{The MIT Press}},
  isbn = {978-0-262-20162-9}
}

@incollection{triggsBundleAdjustmentModern2000,
  title = {Bundle {{Adjustment}} \textemdash{} {{A Modern Synthesis}}},
  booktitle = {Vision {{Algorithms}}: {{Theory}} and {{Practice}}},
  author = {Triggs, Bill and McLauchlan, Philip F. and Hartley, Richard I. and Fitzgibbon, Andrew W.},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Triggs, Bill and Zisserman, Andrew and Szeliski, Richard},
  year = {2000},
  volume = {1883},
  pages = {298--372},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44480-7_21},
  abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
  file = {/home/akashsharma/Zotero/storage/FB7JPG5Z/Triggs et al. - 2000 - Bundle Adjustment — A Modern Synthesis.pdf},
  isbn = {978-3-540-67973-8 978-3-540-44480-0},
  language = {en}
}

@article{tungLearningSpatialCommon2019,
  title = {Learning {{Spatial Common Sense}} with {{Geometry}}-{{Aware Recurrent Networks}}},
  author = {Tung, Hsiao-Yu Fish and Cheng, Ricson and Fragkiadaki, Katerina},
  year = {2019},
  month = apr,
  abstract = {We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to "lift" and integrate 2D visual features over time into latent 3D feature maps of the scene. They are equipped with differentiable geometric operations, such as projection, unprojection, egomotion estimation and stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature state. We train the proposed architectures to predict novel camera views given short frame sequences as input. Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations; they greatly outperform previous works that do not consider egomotion stabilization or a space-aware latent feature state. We train the proposed architectures to detect and segment objects in 3D using the latent 3D feature map as input--as opposed to per frame features. The resulting object detections persist over time: they continue to exist even when an object gets occluded or leaves the field of view. Our experiments suggest the proposed space-aware latent feature memory and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.},
  archiveprefix = {arXiv},
  eprint = {1901.00003},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/I2RFMQYM/Tung et al. - 2019 - Learning Spatial Common Sense with Geometry-Aware .pdf;/home/akashsharma/Zotero/storage/EDUSUPMU/1901.html},
  journal = {arXiv:1901.00003 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/4KJANP28/Vaswani et al. - 2017 - Attention Is All You Need.pdf},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{wangIBRNetLearningMultiView2021,
  title = {{{IBRNet}}: {{Learning Multi}}-{{View Image}}-{{Based Rendering}}},
  shorttitle = {{{IBRNet}}},
  author = {Wang, Qianqian and Wang, Zhicheng and Genova, Kyle and Srinivasan, Pratul and Zhou, Howard and Barron, Jonathan T. and {Martin-Brualla}, Ricardo and Snavely, Noah and Funkhouser, Thomas},
  year = {2021},
  month = feb,
  abstract = {We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods.},
  archiveprefix = {arXiv},
  eprint = {2102.13090},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/UP4I9B4P/Wang et al. - 2021 - IBRNet Learning Multi-View Image-Based Rendering.html},
  journal = {arXiv:2102.13090 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{wangNeRFNeuralRadiance2021,
  title = {{{NeRF}}--: {{Neural Radiance Fields Without Known Camera Parameters}}},
  shorttitle = {{{NeRF}}--},
  author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
  year = {2021},
  month = feb,
  abstract = {This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.},
  archiveprefix = {arXiv},
  eprint = {2102.07064},
  eprinttype = {arxiv},
  journal = {arXiv:2102.07064 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{wengJoint3DTracking2020,
  title = {Joint {{3D Tracking}} and {{Forecasting}} with {{Graph Neural Network}} and {{Diversity Sampling}}},
  author = {Weng, Xinshuo and Yuan, Ye and Kitani, Kris},
  year = {2020},
  month = mar,
  abstract = {3D multi-object tracking (MOT) and trajectory forecasting are two critical components in modern 3D perception systems that require accurate modeling of multi-agent interaction. We hypothesize that it is beneficial to unify both tasks under one framework in order to learn a shared feature representation of agent interaction. To evaluate this hypothesis, we propose a unified solution for 3D MOT and trajectory forecasting which also incorporates two additional novel computational units. First, we propose a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which multiple agents interact with one another. The GNN is able to model complex hierarchical interactions, improve the discriminative feature learning for MOT association, and provide socially-aware context for trajectory forecasting. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efficiently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating many duplicate trajectory samples. We evaluate on the KITTI and nuScenes datasets, showing that our unified method with feature interaction and diversity sampling achieves new state-of-the-art performance on both 3D MOT and trajectory forecasting. Our code will be made available at https://github.com/xinshuoweng/GNNTrkForecast.},
  archiveprefix = {arXiv},
  eprint = {2003.07847},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/869FFLKF/Weng et al. - 2020 - Joint 3D Tracking and Forecasting with Graph Neura.pdf},
  journal = {arXiv:2003.07847 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@inproceedings{whelanElasticFusionDenseSLAM2015,
  title = {{{ElasticFusion}}: {{Dense SLAM Without A Pose Graph}}},
  shorttitle = {{{ElasticFusion}}},
  booktitle = {Robotics: {{Science}} and {{Systems XI}}},
  author = {Whelan, Thomas and Leutenegger, Stefan and Moreno, Renato Salas and Glocker, Ben and Davison, Andrew},
  year = {2015},
  month = jul,
  volume = {11},
  file = {/home/akashsharma/Zotero/storage/Q4F2WEMI/Whelan et al. - 2015 - ElasticFusion Dense SLAM Without A Pose Graph.pdf;/home/akashsharma/Zotero/storage/MIMLFJVE/p01.html;/home/akashsharma/Zotero/storage/WRN963A2/p01.html},
  isbn = {978-0-9923747-1-6}
}

@article{whelanKintinuousSpatiallyExtended,
  title = {Kintinuous: {{Spatially Extended KinectFusion}}},
  author = {Whelan, Thomas and McDonald, John and Kaess, Michael and Fallon, Maurice and Johannsson, Hordur and Leonard, John J},
  pages = {8},
  abstract = {In this paper we present an extension to the KinectFusion algorithm that permits dense mesh-based mapping of extended scale environments in real-time. This is achieved through (i) altering the original algorithm such that the region of space being mapped by the KinectFusion algorithm can vary dynamically, (ii) extracting a dense point cloud from the regions that leave the KinectFusion volume due to this variation, and, (iii) incrementally adding the resulting points to a triangular mesh representation of the environment. The system is implemented as a set of hierarchical multi-threaded components which are capable of operating in real-time. The architecture facilitates the creation and integration of new modules with minimal impact on the performance on the dense volume tracking and surface reconstruction modules. We provide experimental results demonstrating the system's ability to map areas considerably beyond the scale of the original KinectFusion algorithm including a two story apartment and an extended sequence taken from a car at night. In order to overcome failure of the iterative closest point (ICP) based odometry in areas of low geometric features we have evaluated the Fast Odometry from Vision (FOVIS) system as an alternative. We provide a comparison between the two approaches where we show a trade off between the reduced drift of the visual odometry approach and the higher local mesh quality of the ICP-based approach. Finally we present ongoing work on incorporating full simultaneous localisation and mapping (SLAM) pose-graph optimisation.},
  file = {/home/akashsharma/Zotero/storage/CNLPFSWT/Whelan et al. - Kintinuous Spatially Extended KinectFusion.pdf},
  language = {en}
}

@book{williamsConcurrencyActionPractical2012,
  title = {C++ Concurrency in Action: Practical Multithreading},
  shorttitle = {C++ Concurrency in Action},
  author = {Williams, Anthony},
  year = {2012},
  publisher = {{Manning}},
  address = {{Shelter Island, NY}},
  abstract = {Explains how to create multithreaded applications in the program language C++11.--},
  annotation = {OCLC: ocn320189325},
  file = {/home/akashsharma/Zotero/storage/LTELPQL5/Williams - 2012 - C++ concurrency in action practical multithreadin.pdf},
  isbn = {978-1-933988-77-1},
  keywords = {C++ (Computer program language)},
  language = {en},
  lccn = {QA76.73.C153 .W5515 2012}
}

@article{xuLearningInverseDepth2019,
  title = {Learning {{Inverse Depth Regression}} for {{Multi}}-{{View Stereo}} with {{Correlation Cost Volume}}},
  author = {Xu, Qingshan and Tao, Wenbing},
  year = {2019},
  month = dec,
  abstract = {Deep learning has shown to be effective for depth inference in multi-view stereo (MVS). However, the scalability and accuracy still remain an open problem in this domain. This can be attributed to the memory-consuming cost volume representation and inappropriate depth inference. Inspired by the group-wise correlation in stereo matching, we propose an average group-wise correlation similarity measure to construct a lightweight cost volume. This can not only reduce the memory consumption but also reduce the computational burden in the cost volume filtering. Based on our effective cost volume representation, we propose a cascade 3D U-Net module to regularize the cost volume to further boost the performance. Unlike the previous methods that treat multi-view depth inference as a depth regression problem or an inverse depth classification problem, we recast multi-view depth inference as an inverse depth regression task. This allows our network to achieve sub-pixel estimation and be applicable to large-scale scenes. Through extensive experiments on DTU dataset and Tanks and Temples dataset, we show that our proposed network with Correlation cost volume and Inverse DEpth Regression (CIDER), achieves state-of-the-art results, demonstrating its superior performance on scalability and accuracy.},
  archiveprefix = {arXiv},
  eprint = {1912.11746},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/ZHSZEQSE/Xu and Tao - 2019 - Learning Inverse Depth Regression for Multi-View S.pdf;/home/akashsharma/Zotero/storage/N55CSELG/1912.html},
  journal = {arXiv:1912.11746 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{yangGraduatedNonConvexityRobust2020,
  title = {Graduated {{Non}}-{{Convexity}} for {{Robust Spatial Perception}}: {{From Non}}-{{Minimal Solvers}} to {{Global Outlier Rejection}}},
  shorttitle = {Graduated {{Non}}-{{Convexity}} for {{Robust Spatial Perception}}},
  author = {Yang, Heng and Antonante, Pasquale and Tzoumas, Vasileios and Carlone, Luca},
  year = {2020},
  month = apr,
  volume = {5},
  pages = {1127--1134},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.2965893},
  abstract = {Semidefinite Programming (SDP) and Sums-of-Squares (SOS) relaxations have led to certifiably optimal non-minimal solvers for several robotics and computer vision problems. However, most non-minimal solvers rely on least squares formulations, and, as a result, are brittle against outliers. While a standard approach to regain robustness against outliers is to use robust cost functions, the latter typically introduce other non-convexities, preventing the use of existing non-minimal solvers. In this letter, we enable the simultaneous use of non-minimal solvers and robust estimation by providing a general-purpose approach for robust global estimation, which can be applied to any problem where a nonminimal solver is available for the outlier-free case. To this end, we leverage the Black-Rangarajan duality between robust estimation and outlier processes (which has been traditionally applied to early vision problems), and show that graduated non-convexity (GNC) can be used in conjunction with non-minimal solvers to compute robust solutions, without requiring an initial guess. we demonstrate the resulting robust non-minimal solvers in applications, including point cloud and mesh registration, pose graph optimization, and image-based object pose estimation (also called shape alignment). Our solvers are robust to 70-80\% of outliers, outperform RANSAC, are more accurate than specialized local solvers, and faster than specialized global solvers. We also propose thefirst certifiably optimal non-minimal solver for shape alignment using SOS relaxation.},
  file = {/home/akashsharma/Zotero/storage/X4SL6CAD/Yang et al. - 2020 - Graduated Non-Convexity for Robust Spatial Percept.pdf;/home/akashsharma/Zotero/storage/GNX3PNTU/8957085.html},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {Black-Rangarajan duality,computer vision,Computer vision,computer vision problems,Cost function,Estimation,estimation theory,general-purpose approach,global optimization,global outlier rejection,Graduated non-convexity,graduated nonconvexity,graph theory,image registration,image-based object pose estimation,iterative methods,least squares approximations,Measurement uncertainty,mesh registration,minimisation,outlier rejection,point cloud,pose estimation,pose graph optimization,robust estimation,robust nonminimal solvers,robust spatial perception,Shape,shape alignment,SLAM (robots),SOS relaxation,spatial perception},
  number = {2}
}

@article{yangMonocularObjectPlane2019,
  title = {Monocular {{Object}} and {{Plane SLAM}} in {{Structured Environments}}},
  author = {Yang, Shichao and Scherer, Sebastian},
  year = {2019},
  month = oct,
  volume = {4},
  pages = {3145--3152},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2924848},
  abstract = {In this letter, we present a monocular simultaneous localization and mapping (SLAM) algorithm using high-level object and plane landmarks. The built map is denser, more compact and semantic meaningful compared to feature point based SLAM. We first propose a high-order graphical model to jointly infer the three-dimensional object and layout planes from single images considering occlusions and semantic constraints. The extracted objects and planes are further optimized with camera poses in a unified SLAM framework. Objects and planes can provide more semantic constraints such as Manhattan plane and object supporting relationships compared to points. Experiments on various public and collected datasets, including ICL NUIM and TUM Mono show that our algorithm can improve camera localization accuracy compared to state-of-the-art SLAM, especially when there is no loop closure, and also generate dense maps robustly in many structured environments.},
  file = {/home/akashsharma/Zotero/storage/BD4IQKIY/Yang and Scherer - 2019 - Monocular Object and Plane SLAM in Structured Envi.pdf;/home/akashsharma/Zotero/storage/4SWKI8BD/8744612.html},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {camera localization accuracy,cameras,Cameras,dense maps,extracted objects,high-level object,high-order graphical model,ICL NUIM,Layout,layout planes,mobile robots,monocular object,monocular simultaneous localization,Object and Plane SLAM,object supporting relationships,Optimization,plane landmarks,plane SLAM,point based SLAM,Proposals,robot vision,semantic constraints,Semantic Scene Understanding,Semantics,Simultaneous localization and mapping,single images,SLAM,SLAM (robots),structured environments,Three-dimensional displays,three-dimensional object,TUM Mono,unified SLAM framework},
  number = {4}
}

@article{yangTEASERFastCertifiable2020,
  title = {{{TEASER}}: {{Fast}} and {{Certifiable Point Cloud Registration}}},
  shorttitle = {{{TEASER}}},
  author = {Yang, Heng and Shi, Jingnan and Carlone, Luca},
  year = {2020},
  month = jan,
  abstract = {We propose the first fast and certifiable algorithm for the registration of two sets of 3D points in the presence of large amounts of outlier correspondences. Towards this goal, we first reformulate the registration problem using a Truncated Least Squares (TLS) cost that makes the estimation insensitive to spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem is still non-convex and combinatorial in nature, we show that (i) TLS scale and (component-wise) translation estimation can be solved in polynomial time via an adaptive voting scheme, (ii) TLS rotation estimation can be relaxed to a semidefinite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While solving large SDP relaxations is typically slow, we develop a second certifiable algorithm, named TEASER++, that circumvents the need to solve an SDP and runs in milliseconds. For both algorithms, we provide theoretical bounds on the estimation errors, which are the first of their kind for robust registration problems. Moreover, we test their performance on standard benchmarks, object detection datasets, and the 3DMatch scan matching dataset, and show that (i) both algorithms dominate the state of the art (e.g., RANSAC, branch-\&-bound, heuristics) and are robust to more than 99\% outliers, (ii) TEASER++ can run in milliseconds and it is currently the fastest robust registration algorithm, (iii) TEASER++ is so robust it can also solve problems without correspondences (e.g., hypothesizing all-to-all correspondences) where it largely outperforms ICP. We release a fast open-source C++ implementation of TEASER++.},
  archiveprefix = {arXiv},
  eprint = {2001.07715},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/6BESHDBA/Yang et al. - 2020 - TEASER Fast and Certifiable Point Cloud Registrat.pdf;/home/akashsharma/Zotero/storage/88PWGDMN/2001.html},
  journal = {arXiv:2001.07715 [cs, math]},
  keywords = {68T40; 74Pxx; 46N10; 65D19,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,G.1.6,I.2.9,I.4.5,Mathematics - Optimization and Control},
  primaryclass = {cs, math}
}

@incollection{zhouFastGlobalRegistration2016,
  title = {Fast {{Global Registration}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9906},
  pages = {766--782},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46475-6_47},
  abstract = {We present an algorithm for fast global registration of partially overlapping 3D surfaces. The algorithm operates on candidate matches that cover the surfaces. A single objective is optimized to align the surfaces and disable false matches. The objective is defined densely over the surfaces and the optimization achieves tight alignment with no initialization. No correspondence updates or closest-point queries are performed in the inner loop. An extension of the algorithm can perform joint global registration of many partially overlapping surfaces. Extensive experiments demonstrate that the presented approach matches or exceeds the accuracy of state-of-the-art global registration pipelines, while being at least an order of magnitude faster. Remarkably, the presented approach is also faster than local refinement algorithms such as ICP. It provides the accuracy achieved by well-initialized local refinement algorithms, without requiring an initialization and at lower computational cost.},
  file = {/home/akashsharma/Zotero/storage/6DGY2YT7/Zhou et al. - 2016 - Fast Global Registration.pdf},
  isbn = {978-3-319-46474-9 978-3-319-46475-6},
  language = {en}
}

@article{zhouOpen3DModernLibrary2018,
  title = {{{Open3D}}: {{A Modern Library}} for {{3D Data Processing}}},
  shorttitle = {{{Open3D}}},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  year = {2018},
  month = jan,
  abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
  archiveprefix = {arXiv},
  eprint = {1801.09847},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/L3JZ54YP/Zhou et al. - 2018 - Open3D A Modern Library for 3D Data Processing.pdf;/home/akashsharma/Zotero/storage/CI7CHLIH/1801.html},
  journal = {arXiv:1801.09847 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{zouLearningMonocularVisual2020,
  title = {Learning {{Monocular Visual Odometry}} via {{Self}}-{{Supervised Long}}-{{Term Modeling}}},
  author = {Zou, Yuliang and Ji, Pan and Tran, Quoc-Huy and Huang, Jia-Bin and Chandraker, Manmohan},
  year = {2020},
  month = jul,
  abstract = {Monocular visual odometry (VO) suffers severely from error accumulation during frame-to-frame pose estimation. In this paper, we present a self-supervised learning method for VO with special consideration for consistency over longer sequences. To this end, we model the long-term dependency in pose prediction using a pose network that features a two-layer convolutional LSTM module. We train the networks with purely self-supervised losses, including a cycle consistency loss that mimics the loop closure module in geometric VO. Inspired by prior geometric systems, we allow the networks to see beyond a small temporal window during training, through a novel a loss that incorporates temporally distant (e.g., O(100)) frames. Given GPU memory constraints, we propose a stage-wise training mechanism, where the first stage operates in a local time window and the second stage refines the poses with a "global" loss given the first stage features. We demonstrate competitive results on several standard VO datasets, including KITTI and TUM RGB-D.},
  archiveprefix = {arXiv},
  eprint = {2007.10983},
  eprinttype = {arxiv},
  file = {/home/akashsharma/Zotero/storage/S3TYAHHI/Zou et al. - 2020 - Learning Monocular Visual Odometry via Self-Superv.pdf;/home/akashsharma/Zotero/storage/H2JB6PBL/2007.html},
  journal = {arXiv:2007.10983 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}


