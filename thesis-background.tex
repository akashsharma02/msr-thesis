% -*- mode:LaTex; mode:visual-line; mode:flyspell; fill-column:75-*-
\chapter{Background} \label{chap:background}

As eluded to in \ref{chap:introduction}, SLAM systems that run on real robots can be crudely divided into \emph{front-end} and \emph{back-end}. This chapter is a primer on algorithms and methods used in the \emph{front-end} and \emph{back-end}.

\section{The Front End}

The robotic paradigm describes the relationship of an agent interacting with its environment through the \emph{sense-plan-act} control loop. SLAM being in the sense paradigm, deals with processing the raw environment sensor readings to a structured representation that can be used in downstream tasks.

The front end deals with data preparation, modeling and data association such that it is amenable to the underlying optimization. More often than not, the choice of front-end is sensor dependent. For instance, many visual SLAM systems resort to feature based sparse representations, while RGB-Depth sensor based systems resort to direct methods that utilize all the pixels in the input frame. This distinction in the choice of front-end directly influences the choice of map representation in a SLAM system. In particular, a popular choice with visual SLAM system is to use an \emph{explicit} map representation as a collection of 3D points, that is \emph{triangulated} from multiple 2D image feature correspondences, while dense front-end systems, rely on surfel representations or on volumetric representations such as occupancy grids. More recently, after the seminal work \cite{newcombeKinectFusionRealtimeDense2011}, \emph{implicit} map representations have gained popularity as a candidate representation for RGB-Depth SLAM systems.

In this work, since the focus is on higher level object landmarks represented through implicit map representations such as Truncated Signed Distance Function (TSDF) grids, I provide a short introduction of this representation.

\subsection{Implicit map representation through TSDFs}

\emph{Implicit} map representations are fundamentally different from \emph{explicit} map representations. As opposed to point clouds or landmark collections, an implicit map refers to one where the environment is defined through a metric space $\Xv$ and structures in this space are described through an \emph{implicit} function $f : \Xv \rightarrow \Rb$. One example of an implicit function is the log-odds occupancy, resulting directly in occupancy grids \cite{thrunProbabilisticRoboticsIntelligent2005} and \emph{octree} \cite{hornungOctoMapEfficientProbabilistic2013} representations. Another is the signed distance function (SDF) which is relevant for this thesis. Formally, it is defined as follows:

In a metric space $\Xv$ defined with distance $d$, if the surface defined by $\partial \Omega$ demarcates this space into subsets  $\Omegav$ and  $\Omegav^c$, the signed distance function $f$ is defined by
\begin{align*}
   f = \begin{cases}
       d(\xv, \partial \Omegav), &\text{ if } \xv \in \Omegav\\
       -d(\xv, \partial \Omegav), &\text{ if } \xv \in \Omegav^c
   \end{cases}
\end{align*}
In particular, for the 3D environment, we have that $\Xv = \Rb^3$ with $d = \|\cdot, \cdot \|_2$, and $\partial \Omega$ corresponds to the zero level set of the signed distance function and represents the surface of objects as a topological space. In simple words, the signed distance function returns the shortest distance of a query point in euclidean space to its closest point on the surface boundary (\todo{2D SDF illustration} see Figure~\ref{fig:}).

\subsubsection{TSDF integration}

A TSDF representation requires a metric volume by design, which is implemented as a volumetric grid $V$, arbitrarily initialised in the world frame $\Tv_W = \Iv \in SE(3)$. Consider a single RGB-D frame measurement $\langle \Ic, \Dc \rangle$ that consists of a color ($\Ic$) and depth ($\Dc$) image, at a relative camera pose $\Tv_C^W  = \begin{bmatrix}
    \Rv_C^W & \tv_C^W \\ 0 & 1
\end{bmatrix}\in SE(3)$ with camera intrinsics $\Kv$. To integrate the measurement into the TSDF volume, we project  all the query points $q \in \Rb^3$ of the volume $V$ into the current image as follows:
\begin{align*}
    \dot{q}^\prime = (\Tv_C^W)^{-1} \dot{q} \\
    \dot{p} = \lfloor \Kv q^\prime\rfloor
\end{align*}
where $\dot{q} = \begin{bmatrix}
    q^\top & 1
\end{bmatrix}^\top$  and similarly $\dot{p}$ denote the homogeneous coordinates in $\Rb^2$ and $\Rb^3$. $p \in \Zb^2$ is the associated 2D pixel coordinate in the image and $\lfloor \cdot \rfloor$ finds the nearest integer pixel coordinate in the image. For every matched pixel coordinate, we backproject the associated measurement i.e., the depth value of the pixel $\Dc(p) \in \Rb$ into the query coordinate $q$ as follows:
\begin{align*}
    \text{SDF}(q) =  \norm{ \Dc(p) - \frac{1}{\lambda} \|\tv_C^W - q\|_2 }_2
\end{align*}
Where $\| \tv_C^W - q \|_2$ is the distance between the query point and the camera center along the camera optical axis, and $\lambda = K^{-1} p$ defines the ray direction for the pixel $p$. This effectively calculates the shortest distance of the query coordinate to the surface.

This SDF value is truncated such that only a band of SDF values ($+\mu$ to $-\mu$) around the measured surface are computed, to avoid computation of all the query coordinates in the volume.
Similarly, colors from the image, can be associated to the query coordinates, by unprojection uniquely since a depth image is available.

Now, when we have multiple RGB-D frame measurements $\{\langle \Ic^{(i)}, \Dc^{(i)} \rangle\}_{i=1}^{N}$ with their respective poses $\{ T_{C^{(i)}}^W \}_{i=1}^N$, we can compute TSDF measurements from each individual frame. The global fusion of all these TSDF measurements is then done through weighted average of the TSDF measurements for each query coordinate $q \in \Rb^3$ over the camera frame sequence as follows:
\begin{align*}
    d &= \phi(\text{SDF}(q)) \\
    \text{TSDF}_k(q) &= \frac{\text{TSDF}_{k-1}(q) + d}{W(q) + 1} \\
    W(q) &= W(q) + 1 \\
\end{align*}
Where $\phi$ truncates the $\text{SDF}$ computed for the frame being integrated, and $W(q)$ corresponds to the maintained weight for every voxel coordinate in the volume grid.

In general, the integration process can also be applied to the color values, where the colors are averages elementwise, with the incoming stream of images.

\subsubsection{TSDF Rendering}

TSDF Rendering is the process of generating a virtual depth and color image given a camera pose $T_C^W$ and TSDF volume grid $V$. The previous subsection assumed access to camera poses for global TSDF fusion, but in an actual SLAM setting camera poses are computed in the loop typically through iterative closest point (ICP) registration. TSDF Rendering is the intermediate step that generates a virtual image that the incoming camera frame is registered against (also called \emph{frame to model} registration~\cite{newcombeKinectFusionRealtimeDense2011}). (details in \S\ref{subsec: tracking})

This is accomplished by marching a ray per pixel of the virtual frame, as we have a global TSDF volume, which encodes surfaces as the zero level set (zero crossing). For a given pixel $p \in \Zb^2$, once again the camera ray can be computed as
\begin{align*}
    \rv = \Tv_C^W \Kv^{-1} \dot{p}
\end{align*}

We then march along the ray from the minimum depth supported by the camera (usually assumed to be $0.1$m) to a max depth value defined as the supported camera range, or until we find a zero crossing. Typically this process is sped up by skipping marching along the ray, and using a step size $< \mu$ the truncation threshold. Once a zero crossing has been found, the depth value can be further refined by trilinear interpolation. (details in~\cite{newcombeKinectFusionRealtimeDense2011})

\subsection{TSDF Limitations}

Based on the discussion of TSDF for map representation in SLAM, the following limitations might be apparent to the keen reader.
\begin{enumerate}
    \item \textbf{TSDF representation is memory intensive:} A naive TSDF representation, utilizes a 3D volumetric grid to represent the scene -- typically  a voxel grid of resolution $512^3$ covering a bounded volume of about $4m^3$. This makes the entire system very memory intensive. Later works such as Voxel Hashing~\cite{niessnerRealtime3DReconstruction2013} and OctoMap \cite{hornungOctoMapEfficientProbabilistic2013} have tried to ameliorate this issue. In our implementation, we utilize a spatial hashing based TSDF implementation based on \cite{dongGPUAcceleratedRobust2019} to achieve memory efficiency.
    \item \textbf{TSDFs are computationally intensive:} During integration of an incoming frame, all pixels that are projectively associated to the 3D query coordinates are updated requiring about $640\times 480 = 307200$ for a VGA resolution image. Similarly, during TSDF rendering, each pixel to be rendered requires at most $\frac{\|r\|_2}{\mu}$ evaluations, where $\|r\|_2$ is the ray length (at most the length of TSDF volume cube diagonal), and $\mu$ is the TSDF truncation width. Therefore, ray-casting is typically the most computationally intensive step in any dense RGB-D slam system. Observing that each voxel coordinate is updated independent of every other voxel, a GPU can (and is) used to parallelize all TSDF operations, to obtain realtime operation.
    \item \textbf{Cannot easily smooth noisy pose estimates:} It is not easy to correct noisy pose estimates with implicit models, and therefore most early systems employed only a filtering based integration \cite{newcombeKinectFusionRealtimeDense2011}. This is because to correct for a noisy pose estimate in retrospect, requires the expensive operation of subtracting the weighted TSDF values in the global volume from the appropriate coordinates, and re-integrating the volume with corrected TSDF values \cite{daiBundleFusionRealtimeGlobally2017}. \cite{whelanKintinuousSpatiallyExtended} used submaps to subvert this problem, however with large TSDF volumes as submaps there are still locally noisy measurements that are not corrected for. In our method, since we use a TSDF representation only for landmark objects, we reap the benefits of explicit representation, where the whole landmark pose can be optimized, but each landmark provides a dense and appealing representation. (see Figure~\ref{fig:})
\end{enumerate}

\section{The Back End}

The back end is responsible for generating a reasonable explanation in the state variables for the abstract sensor measurements and data association provided from the front end. As opposed to filtering based methods such as Kalman Filter, more recently smoothing formulations based on matrix factorization have provided exact solutions (in the linear case) by adding the entire trajectory into the optimization problem while simplifying the solution~\cite{kaessIncrementalSmoothingMapping}. In this section, I review the batch smoothing formulation that is quite common in graph based SLAM back-ends today~\cite{grisettiTutorialGraphBasedSLAM, dellaertFactorGraphsRobot2017}.

\subsection{SLAM and non-linear least squares}

As introduced in \ref{chap:introduction}, by treating the robot states and landmarks as random variables given measurements, the SLAM problem can be treated as the maximum a posteriori (MAP) estimation of robot and landmark states. Now, consider a collection of robot state variables $\Xc \triangleq \{ \xv_t \}_{t=1}^{T}$, and a collection of landmarks $\Lc \triangleq \{ \ell_m \}_{m=1}^{M}$, and set of measurements $\Zc \triangleq \{\{\zv_n \}_{n=1}^{N}, \{\zv_t\}_{t=1}^{T}\}$ including both landmark and odometry measurements respectively, then the MAP estimation is given as
\begin{align}
    \Xc^*, \Lc^* = \argmax_{\Xc, \Lc} p(\Xc, \Lc | \Zc) = \argmax_{\Xc, \Lc} p(\Zc | \Xc, \Lc) p(\Xc, \Lc) \label{eq:2.1}
\end{align}

While MAP estimation in general is known to be NP-Hard, by assuming reasonable conditional independences between the associated variables (see Figure~\ref{fig:}), i.e., by making the markovian assumption on the state variables, and observing that measurements are local, we can write the joint probability in \ref{eq:2.1} as follows:
\begin{align}
    p(\Xc, \Lc, \Zc) &= p(\xv_0) \prod_{n=1}^{N} p(\zv_n | \xv_{\alpha_n}, \ell_{\beta_n}) \prod_{t=1}^{T} p(\zv_t | \xv_{t-1}, \xv_{t})\nonumber\\
    \implies \log p(\Xc, \Lc, \Zc) &= \log p(\xv_0) + \sum_{n=1}^{N} \log p(\zv_n | \xv_{\alpha_n}, \ell_{\beta_n}) + \sum_{t=1}^{T} \log p(\zv_t | \xv_{t-1}, \xv_{t})
\end{align}
Note, that the above problem assumes known \emph{data association} between measurements $\zv_n$ of landmark $\ell_{\beta_n}$ at a robot pose $\xv_{\alpha_n}$as $\Dc := \{(\alpha_n, \beta_n)\}_{n=1}^{N}$ \cite{bowmanProbabilisticDataAssociation2017}.
Now as is typical in SLAM, we assume gaussian measurement noise in each of the measurements i.e.,
\begin{align*}
    \zv_0 &= \xv_0 + \nuv_0\\
    \zv_n &= h_n(\xv_{\alpha_n}, \ell_{\beta_n}) + \nuv_n\\
    \zv_t &= h_t(\xv_{t-1}, \xv_t) ~+ \nuv_t
\end{align*}
Where $\nuv_n$ and $\nuv_t$ are the intrinsic noise in the respective measurements, each distributed as zero mean noise with respective covariances $\Sigma_n$ and $\Sigma_t$. Also note that we added a phantom measurement $\zv_0$ to constrain the first pose with an empirically chosen small covariance $\Sigma_0$. On writing down the probability distributions for each of the gaussian noise terms we have the following:
\begin{align*}
    p(\xv_0) &\propto \exp\{ \|\zv_0 - \xv_0\|^2_{\Sigma_0}\}\\
    p(\zv_n | \xv_{\alpha_n}, \ell_{\beta_n}) &\propto \exp\{ \|\zv_n - h_n(\xv_{\alpha_n}, \ell_{\beta_n})\|^2_{\Sigma_n}\}\\
    p(\zv_t | \xv_{t-1}, \ell_{t}) &\propto \exp\{ \|\zv_t - h_n(\xv_{t-1}, \ell_{t})\|^2_{\Sigma_t}\}
\end{align*}
Observe that
\begin{align*}
    \Xc^*, \Lc^* &= \argmax_{\Xc, \Lc} p(\Xc, \Lc, \Zc) = \argmin_{\Xc, \Lc} - \log p(\Xc, \Lc, \Zc)
\end{align*}
Now writing the joint probability using the factorized measurement distributions we have
\begin{align*}
    \Xc^*, \Lc^* = \argmin_{\Xc, \Lc} \Bigg \{ \| \zv_0 - \xv_0 \|_{\Sigma_0}^2 + &\sum_{n=1}^{N} \| \zv_n - h_n(\xv_{\alpha_n}, \ell_{\beta_n}) \|_{\Sigma_t}^2 + \\&\sum_{t=1}^{T}\| \zv_t - h_t(\xv_{t-1}, \xv_{t}) \|_{\Sigma_n}^2 \Bigg \}
\end{align*}
If we generalize the different types of measurements in the system as $h_i(\Xv_i)$, and subsume the landmarks and state variables into a single state vector $\Xc$ then we may write the above as:
\begin{align}
    \Xc^* = \argmin_{\Xc} \Bigg \{ &\sum_{i=1}^{N} \| \zv_i - h_i(\Xv_i) \|_{\Sigma_i}^2 \Bigg \} \label{eq:2.3}
\end{align}
The above minimization is a non-linear least squares optimization which is typically solved by iteratively linearizing the measurement models and solving a series of linear approximations to the problem to approach a local minima solution for landmark and robot latent state variables.

\subsection{Iterative Linearization and Gauss Newton}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/linearization}
    \caption{Illustration of linearization using taylor's expansion\todo{Fix}}
    \label{fig:taylor}
\end{figure}

Consider a single non-linear measurement $h_i( \cdot )$, then by taylor's expansion, we can linearize the measurement at a linearization point $\Xv_i^{(0)}$ as follows (see Figure~\ref{fig:taylor}):
\begin{align*}
    h_i(\Xv_i) \approx h_i(\Xv_i^{(0)}) + \frac{\partial h_i}{\partial \Xv_i} \Bigg |_{\Xv_i^{(0)}} (\Xv_i - \Xv_i^{(0)})
\end{align*}
If we define the jacobian as $\Jv_i \triangleq \frac{\partial h_i}{\partial \Xv_i} \left|_{\Xv_i^{(0)}}$, and $\Delta_i \triangleq (\Xv_i - \Xv_i^{(0)})$ and substitute in the general non-linear least squares objective \ref{eq:2.3}:
\begin{align*}
    \Delta^* = \argmin_{\Delta}\sum_{i} \| \{\zv_i - h_i(X_i^{(0)})\} - \Jv_i \Delta_i \|_{\Sigma_i}^2
\end{align*}
where $\qv_i = \zv_i - h_i(\Xv_i^{(0)})$ is the prediction error at the linearization point, and $\Delta^*$ is the solution to the locally linearized problem.
\begin{align*}
    \Delta^* &= \argmin_{\Delta}\sum_{i} \| \qv_i - \Jv_i \Delta_i \|_{\Sigma_i}^2 = \argmin_{\Delta}\sum_{i} (\qv_i - \Jv_i \Delta_i)^\top \Sigma_i^{-1} (\qv_i - \Jv_i \Delta_i)\\
             &=  \argmin_{\Delta}\sum_{i} \left\{\Sigma_i^{-1/2}(\qv_i - \Jv_i \Delta_i)\right\}^\top \left\{ \Sigma_i^{-1/2} (\qv_i - \Jv_i \Delta_i)\right\}\\
             &= \argmin_{\Delta}\sum_{i} \| \bv_i - \Av_i \Delta_i \|_2^2 = \argmin_{\Delta} \| \bv - \Av \Delta\|_2^2
\end{align*}
By writing $\bv_i = \Sigma_i^{-1/2} \qv_i$ and $\Av_i = \Sigma_i^{-1/2} \Jv_i$ and then collecting them in the matrix $\bv$ and $\Av$ respectively, we obtain the standard least squares problem, that can then be solved using \emph{normal equations}.

The above solution gives us the local update $\Delta^*$ for a linearization point $\Xv^{(0)}$. To solve a non-linear problem, we solve for local updates iteratively as in Algorithm~\ref{gauss-newton}
\input{algorithms/gauss-newton.tex}

A more detailed treatment of sophisticated non-linear optimization algorithms in the context of SLAM is provided in \cite{dellaertFactorGraphsRobot2017}.

\subsection{Discussion and Limitations}

We noted earlier that MAP inference on the factor graph for the SLAM setting is tractable due to the inherent local structure of the factor graph. This structure appears in the sparsity of the measurement jacobian $\Av = \Sigma^{-1/2} \Jv$ with number of rows equal to the number of measurements in the graph (number of factors) and the number of columns are the number of state variables to be solved. The non-linear least squares optimization is solved efficiently using matrix factorization methods such as QR factorization and cholesky factorization (equivalent to variable elimination on the factor graph~\cite{dellaertSquareRootSAM2006}). Further improvement in performance can be gained by reordering the measurements using methods such as COLAMD \cite{davisAlgorithm8xxCOLAMD} which consequently reduces fill in in the matrix during computation.

Nevertheless, for an online system with this formulation matrix factorization needs to be performed with a new graph when new measurements are added, resulting in wasted computation to resolve all the entries in the matrix. Later work has made progress on incremental solvers for SLAM, that are able to update the solution iteratively for non-linear systems and provide linearization point management, as well as careful updating of older variables, based on the current estimate. As this thesis does not directly use incremental solvers, the reader is directed to \citet{kaessISAM2IncrementalSmoothing2012}

For implicit mapping representations however, it is unclear how they are to be represented as landmark variables, and how updates to the landmark state variables are to be propagated in the map. In this thesis, we show that representing the map as a collection of high level objects, each corresponding to a landmark variable in the optimization is a good strategy, and benefits the SLAM in multiple facets. Each object is represented with a spatially hashed TSDF volumetric grid \cite{prisacariuInfiniTAMV3Framework2017} \cite{niessnerRealtime3DReconstruction2013} \cite{dongGPUAcceleratedRobust2019} that is not updated in the back end optimization process. Instead, since the object is rigid, optimize the pose of the base frame attached to it suffices


% In theory implicit map representations provide a method for water tight  reconstructions that are visually appealing, and in general more detailed when compared to feature based methods (see Figure Figure~\ref{fig:}), however these methods typically do not employ smoothing based back ends, and therefore are unable to correct for noisy pose estimates in retrospect. This is because, it is not easy to make a correction to an integrated measurement in the TSDF representation. In particular, to correct a single pose estimate, the measurement that had been integrated into the global TSDF has to be removed by subtractingthe corresponding TSDF value from the affected 3D query coordinates, and then re-integrating the new measurement appropriately.

